I did a fine tune training run restricitng myself to my m2 macbook air. I used Qwen2.5-1.5B-instruct and it trained for 8 hours on ~3.22 million tokens

Learning goals:
- understand LoRA configuration
- batch size vs gradient accumulation
- Experience real loss spike + recovery
- Learn MLX optimization
- Practice dataset creation
- Perform a complete ML training pipeline
