{"text": "Question: What is He initialization?\nAnswer: He initialization uses variance 2/n_in, doubling Xavier's variance. It's designed for ReLU activations which kill half the gradients. This compensation maintains stable gradient flow through deep ReLU networks."}
{"text": "Q: What is mixed precision training?\nA: Mixed precision training uses both FP16 (fast, memory-efficient) and FP32 (stable, precise) arithmetic. Activations and gradients use FP16 while master weights stay in FP32. This accelerates training and reduces memory with minimal quality loss."}
{"text": "What is k-fold cross-validation?\n\nK-fold cross-validation divides data into k equal parts (folds). The model trains k times, each time using a different fold for validation and the remaining k-1 for training. Common choices are k=5 or k=10."}
{"text": "User: What is tokenization?\nAssistant: Tokenization breaks text into smaller units (tokens) such as words, subwords, or characters. It's the first step in NLP pipelines, converting raw text into structured units that models can process numerically."}
{"text": "What is padding in CNNs?\n\nPadding adds border pixels (usually zeros) around inputs before convolution. This preserves spatial dimensions, allows filters to process edge pixels effectively, and prevents rapid shrinking of feature maps through network depth."}
{"text": "Question: Why is LoRA effective?\nAnswer: LoRA is effective because weight updates during fine-tuning often have low intrinsic dimensionality. By constraining updates to low-rank subspaces, it captures essential adaptations while dramatically reducing parameters. This also acts as regularization, preventing overfitting."}
{"text": "User: Explain the difference between classification and regression.\nAssistant: Classification predicts discrete categories or labels (e.g., cat vs dog), while regression predicts continuous numerical values (e.g., temperature). Both are supervised learning tasks but differ in output type and evaluation metrics."}
{"text": "Question: What are the disadvantages of LoRA?\nAnswer: LoRA adds inference latency if not merged with base weights, requires careful hyperparameter tuning (rank, alpha), and may underperform full fine-tuning on tasks requiring substantial behavior changes. The optimal rank varies across tasks and models."}
{"text": "What is mean squared error?\n\nMean squared error (MSE) averages the squared differences between predictions and targets. It's the standard loss function for regression, penalizing large errors more heavily than small ones due to squaring."}
{"text": "Question: What is the transformer architecture?\nAnswer: Transformers process sequences using self-attention mechanisms instead of recurrence. They enable parallel computation, capture long-range dependencies efficiently, and form the foundation of modern NLP models like BERT, GPT, and T5."}
{"text": "Q: What is layer normalization?\nA: Layer normalization normalizes across features for each sample independently, unlike batch norm which normalizes across the batch. It's useful for RNNs and transformers where batch sizes vary or batch statistics are unreliable."}
{"text": "Question: What is hyperparameter tuning?\nAnswer: Hyperparameter tuning systematically searches for optimal hyperparameter values. Methods include grid search (exhaustive), random search (efficient), and Bayesian optimization (smart). Good tuning can dramatically improve model performance."}
{"text": "What is dropout?\n\nDropout randomly deactivates neurons during training with probability p (typically 0.5). This prevents co-adaptation of features, creates an ensemble effect by training different sub-networks, and serves as powerful regularization for deep networks."}
{"text": "Question: What is feature engineering?\nAnswer: Feature engineering is the process of creating new features or transforming existing ones to improve model performance. It involves domain knowledge to extract meaningful representations that make patterns more apparent to learning algorithms."}
{"text": "User: What is He initialization?\nAssistant: He initialization uses variance 2/n_in, doubling Xavier's variance. It's designed for ReLU activations which kill half the gradients. This compensation maintains stable gradient flow through deep ReLU networks."}
{"text": "Explain the curse of dimensionality.\n\nThe curse of dimensionality refers to phenomena that arise when working with high-dimensional data. As dimensions increase, data becomes sparse, distances lose meaning, and exponentially more samples are needed to maintain density and statistical significance."}
{"text": "Q: What is feature selection?\nA: Feature selection identifies and retains only the most relevant features for a task, removing irrelevant or redundant ones. This reduces dimensionality, improves model interpretability, decreases training time, and can prevent overfitting."}
{"text": "Q: What is batch normalization?\nA: Batch normalization normalizes layer inputs across mini-batches to have zero mean and unit variance. This reduces internal covariate shift, stabilizes training, allows higher learning rates, and acts as a regularizer, significantly accelerating deep network training."}
{"text": "User: What is QLoRA?\nAssistant: QLoRA (Quantized LoRA) combines LoRA with 4-bit quantization of base model weights. It enables fine-tuning of very large models on consumer GPUs by drastically reducing memory requirements while maintaining competitive performance through careful quantization and LoRA adaptation."}
{"text": "User: What is accuracy?\nAssistant: Accuracy is the fraction of correct predictions: (TP + TN) / Total. While intuitive, it's misleading for imbalanced datasets where a naive baseline achieves high accuracy by always predicting the majority class."}
{"text": "User: Explain the ReLU activation function.\nAssistant: ReLU (Rectified Linear Unit) outputs max(0, x), zero for negative inputs and identity for positive. It's computationally efficient, mitigates vanishing gradients, and induces sparsity. It's the most popular activation for hidden layers in deep networks."}
{"text": "What is bagging?\n\nBagging (Bootstrap Aggregating) trains multiple models on different random samples of training data (with replacement) and averages their predictions. It reduces variance and prevents overfitting, with Random Forests being a popular example."}
{"text": "What is the confusion matrix?\n\nA confusion matrix tabulates classification results showing true positives, false positives, true negatives, and false negatives. It provides detailed insight into model performance, revealing which classes are confused and error patterns."}
{"text": "User: What is batch normalization?\nAssistant: Batch normalization normalizes layer inputs across mini-batches to have zero mean and unit variance. This reduces internal covariate shift, stabilizes training, allows higher learning rates, and acts as a regularizer, significantly accelerating deep network training."}
{"text": "Q: What is average pooling?\nA: Average pooling computes the mean of values within each pooling window. It provides smoother downsampling than max pooling and is sometimes used in network final layers to aggregate spatial information."}
{"text": "Question: What is catastrophic forgetting?\nAnswer: Catastrophic forgetting occurs when a neural network abruptly loses previously learned knowledge upon learning new information. It's a major challenge in continual learning, as updating weights for new tasks can overwrite weights important for old tasks."}
{"text": "Question: What is boosting?\nAnswer: Boosting sequentially trains weak learners, each focusing on examples the previous models struggled with. It combines these weak learners into a strong ensemble that often achieves excellent performance. Examples include AdaBoost, Gradient Boosting, and XGBoost."}
{"text": "Q: What are the disadvantages of LoRA?\nA: LoRA adds inference latency if not merged with base weights, requires careful hyperparameter tuning (rank, alpha), and may underperform full fine-tuning on tasks requiring substantial behavior changes. The optimal rank varies across tasks and models."}
{"text": "Q: What is QLoRA?\nA: QLoRA (Quantized LoRA) combines LoRA with 4-bit quantization of base model weights. It enables fine-tuning of very large models on consumer GPUs by drastically reducing memory requirements while maintaining competitive performance through careful quantization and LoRA adaptation."}
{"text": "What is machine learning?\n\nMachine learning is a branch of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed. It uses algorithms to identify patterns in data and make predictions or decisions based on those patterns."}
{"text": "Question: What is Bayesian optimization?\nAnswer: Bayesian optimization builds a probabilistic model of the objective function and uses it to select promising hyperparameters to evaluate next. It balances exploration and exploitation, efficiently finding good configurations with fewer expensive evaluations."}
{"text": "User: What are the disadvantages of LoRA?\nAssistant: LoRA adds inference latency if not merged with base weights, requires careful hyperparameter tuning (rank, alpha), and may underperform full fine-tuning on tasks requiring substantial behavior changes. The optimal rank varies across tasks and models."}
{"text": "User: What is BLEU score?\nAssistant: BLEU (Bilingual Evaluation Understudy) evaluates machine translation quality by comparing n-gram overlap between generated and reference translations. It ranges from 0 to 100, with higher scores indicating better translation quality."}
{"text": "Question: What is the attention mechanism?\nAnswer: Attention allows models to focus on relevant input parts when processing each output. It computes weighted combinations of inputs based on learned relevance scores, enabling models to handle variable-length sequences and capture long-range dependencies effectively."}
{"text": "Question: Explain stride in convolution.\nAnswer: Stride determines the step size when sliding filters across inputs. Stride 1 moves one pixel at a time; stride 2 skips every other position. Larger strides reduce output dimensions and computational cost but may miss fine details."}
{"text": "Q: What is the sigmoid activation function?\nA: The sigmoid function maps inputs to (0,1) using the formula 1/(1+e^-x). It's useful for binary classification output layers and was historically popular for hidden layers, though it suffers from vanishing gradients."}
{"text": "Question: What is a hyperparameter?\nAnswer: Hyperparameters are configuration settings that control the learning process but aren't learned from data. Examples include learning rate, batch size, number of layers, and regularization strength. They must be set before training and tuned for optimal performance."}
{"text": "Explain convolutional layers.\n\nConvolutional layers apply learnable filters (kernels) to input feature maps through convolution operations. Each filter detects specific patterns (edges, textures, objects) and produces feature maps showing where those patterns occur spatially."}
{"text": "What is a training set?\n\nA training set is the portion of data used to train a machine learning model. The model learns patterns, relationships, and parameters from this data. It's typically the largest split, often 60-80% of total data."}
{"text": "User: What is transfer learning in computer vision?\nAssistant: Transfer learning uses models pre-trained on large datasets (like ImageNet) as starting points for new tasks. The network has already learned useful features, requiring less data and training time for downstream tasks through fine-tuning."}
{"text": "What are feature maps?\n\nFeature maps are the outputs of convolutional or pooling layers, representing the presence and strength of learned features at different spatial locations. Early layers detect simple features (edges), while deeper layers detect complex patterns (objects)."}
{"text": "Explain the perceptron.\n\nThe perceptron is the simplest neural network unit, computing a weighted sum of inputs plus a bias, then applying a step activation function. While limited to linear decision boundaries, it forms the foundation of modern neural networks."}
{"text": "User: What is a validation set?\nAssistant: A validation set is data held out from training to tune hyperparameters and make model selection decisions. It provides unbiased evaluation during development without touching the test set, preventing overfitting to test data."}
{"text": "Q: What is a test set?\nA: A test set is data completely held out from training and validation, used only for final model evaluation. It provides an unbiased estimate of how the model will perform on new, unseen data in production."}
{"text": "Q: What is hyperparameter tuning?\nA: Hyperparameter tuning systematically searches for optimal hyperparameter values. Methods include grid search (exhaustive), random search (efficient), and Bayesian optimization (smart). Good tuning can dramatically improve model performance."}
{"text": "Question: What is a neural network?\nAnswer: A neural network is a computational model inspired by biological neurons, consisting of interconnected nodes (artificial neurons) organized in layers. It learns to map inputs to outputs through adjustable weights that are optimized during training."}
{"text": "Q: What is an activation function?\nA: An activation function introduces non-linearity into neural networks, enabling them to learn complex patterns. It determines whether and how strongly a neuron fires based on its input. Examples include ReLU, sigmoid, and tanh."}
{"text": "User: What is gradient descent?\nAssistant: Gradient descent is an iterative optimization algorithm that adjusts parameters in the direction of steepest descent of the loss function. It uses gradients (derivatives) to determine update direction and magnitude, moving toward a local minimum."}
{"text": "Q: Explain stride in convolution.\nA: Stride determines the step size when sliding filters across inputs. Stride 1 moves one pixel at a time; stride 2 skips every other position. Larger strides reduce output dimensions and computational cost but may miss fine details."}
{"text": "User: What is ResNet?\nAssistant: ResNet introduces skip connections (residual connections) that add layer inputs directly to outputs. This enables training very deep networks (50-152+ layers) by allowing gradients to flow directly through the network, solving degradation problems."}
{"text": "Question: Explain Word2Vec.\nAnswer: Word2Vec learns word embeddings by predicting words from context (CBOW) or context from words (Skip-gram). It trains a shallow neural network on large corpora, producing embeddings where semantically similar words cluster together in vector space."}
{"text": "Question: What is a training set?\nAnswer: A training set is the portion of data used to train a machine learning model. The model learns patterns, relationships, and parameters from this data. It's typically the largest split, often 60-80% of total data."}
{"text": "User: What is the sigmoid activation function?\nAssistant: The sigmoid function maps inputs to (0,1) using the formula 1/(1+e^-x). It's useful for binary classification output layers and was historically popular for hidden layers, though it suffers from vanishing gradients."}
