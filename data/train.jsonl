{"text": "Q: What is backpropagation?\nA: Backpropagation is the algorithm for training neural networks. It computes gradients of the loss function with respect to each weight by propagating errors backward through the network using the chain rule, enabling gradient descent optimization."}
{"text": "What is gradient accumulation?\n\nGradient accumulation computes gradients over multiple mini-batches before updating weights. This simulates larger batch sizes without increased memory, as gradients are summed across batches then averaged for the update. Useful for memory-constrained training."}
{"text": "What is the F1 score?\n\nThe F1 score is the harmonic mean of precision and recall: 2 \u00d7 (precision \u00d7 recall) / (precision + recall). It balances both metrics, useful when you need a single score accounting for both false positives and false negatives."}
{"text": "Question: What is stacking?\nAnswer: Stacking (Stacked Generalization) trains a meta-model to combine predictions from multiple base models. The meta-model learns optimal weights or combinations of base predictions, often outperforming simple averaging."}
{"text": "Explain supervised learning.\n\nSupervised learning is a machine learning paradigm where models learn from labeled training data. The algorithm learns a mapping from inputs to outputs using examples, then applies this learned mapping to make predictions on new, unseen data. Common tasks include classification and regression."}
{"text": "Q: Explain the perceptron.\nA: The perceptron is the simplest neural network unit, computing a weighted sum of inputs plus a bias, then applying a step activation function. While limited to linear decision boundaries, it forms the foundation of modern neural networks."}
{"text": "Q: What is mean squared error?\nA: Mean squared error (MSE) averages the squared differences between predictions and targets. It's the standard loss function for regression, penalizing large errors more heavily than small ones due to squaring."}
{"text": "Question: What is overfitting?\nAnswer: Overfitting occurs when a model learns training data too well, including noise and outliers. The model memorizes rather than generalizes, achieving high training accuracy but poor performance on new data. Signs include large gap between train and validation performance."}
{"text": "Question: What is a filter/kernel?\nAnswer: A filter (or kernel) is a small matrix of learnable weights that slides across inputs in convolutional layers. It detects specific patterns through element-wise multiplication and summation, producing high responses where patterns match."}
{"text": "Question: What is underfitting?\nAnswer: Underfitting happens when a model is too simple to capture underlying data patterns. It performs poorly on both training and test data because it lacks the capacity to learn the true relationship between inputs and outputs."}
{"text": "Question: What is object detection?\nAnswer: Object detection identifies and localizes multiple objects in images, predicting both class labels and bounding boxes. It's more challenging than classification as it requires determining what objects exist and where they are located."}
{"text": "What is grid search?\n\nGrid search exhaustively evaluates model performance across all combinations of predefined hyperparameter values. While thorough, it's computationally expensive and suffers from the curse of dimensionality as the number of hyperparameters grows."}
{"text": "Question: What is feature selection?\nAnswer: Feature selection identifies and retains only the most relevant features for a task, removing irrelevant or redundant ones. This reduces dimensionality, improves model interpretability, decreases training time, and can prevent overfitting."}
{"text": "Question: What is max pooling?\nAnswer: Max pooling selects the maximum value within each pooling window, retaining the strongest activation. It provides translation invariance, reduces dimensions, and helps the network focus on whether features are present rather than their exact location."}
{"text": "User: What is machine translation?\nAssistant: Machine translation automatically translates text from one language to another. Modern neural machine translation uses encoder-decoder architectures with attention, achieving near-human performance for some language pairs."}
{"text": "Q: What is Bayesian optimization?\nA: Bayesian optimization builds a probabilistic model of the objective function and uses it to select promising hyperparameters to evaluate next. It balances exploration and exploitation, efficiently finding good configurations with fewer expensive evaluations."}
{"text": "Question: What is the learning rate?\nAnswer: The learning rate controls the step size in gradient descent optimization. Too high causes divergence or oscillation around minima; too low results in slow convergence or getting stuck. It's one of the most critical hyperparameters requiring careful tuning."}
{"text": "Question: What is sentiment analysis?\nAnswer: Sentiment analysis determines the emotional tone or opinion expressed in text (positive, negative, neutral). It's widely used for analyzing customer feedback, social media monitoring, and market research."}
{"text": "Question: How do you merge LoRA weights?\nAnswer: LoRA weights can be merged into base model weights after training: W_merged = W_base + \u03b1 \u00d7 A \u00d7 B. This eliminates inference overhead from adapter modules while preserving task-specific adaptations in a single model that can be deployed normally."}
{"text": "Question: What is padding in CNNs?\nAnswer: Padding adds border pixels (usually zeros) around inputs before convolution. This preserves spatial dimensions, allows filters to process edge pixels effectively, and prevents rapid shrinking of feature maps through network depth."}
{"text": "User: Explain transfer learning.\nAssistant: Transfer learning applies knowledge from models trained on one task to improve performance on different but related tasks. Pre-trained models capture useful features and representations that transfer across domains, enabling efficient learning with limited data."}
{"text": "Question: What is natural language processing?\nAnswer: Natural language processing (NLP) enables computers to understand, interpret, and generate human language. It combines linguistics, computer science, and machine learning to process text and speech for tasks like translation, sentiment analysis, and question answering."}
{"text": "What is precision?\n\nPrecision is the fraction of true positives among all positive predictions: TP / (TP + FP). It answers 'Of all instances predicted positive, how many were actually positive?' High precision means few false positives."}
{"text": "Q: What is data augmentation in images?\nA: Data augmentation artificially increases training data diversity by applying transformations like rotations, flips, crops, color jitter, and noise. This improves model robustness, reduces overfitting, and teaches invariance to common variations."}
{"text": "What is a filter/kernel?\n\nA filter (or kernel) is a small matrix of learnable weights that slides across inputs in convolutional layers. It detects specific patterns through element-wise multiplication and summation, producing high responses where patterns match."}
{"text": "What is sentiment analysis?\n\nSentiment analysis determines the emotional tone or opinion expressed in text (positive, negative, neutral). It's widely used for analyzing customer feedback, social media monitoring, and market research."}
{"text": "Question: What is gradient accumulation?\nAnswer: Gradient accumulation computes gradients over multiple mini-batches before updating weights. This simulates larger batch sizes without increased memory, as gradients are summed across batches then averaged for the update. Useful for memory-constrained training."}
{"text": "Question: How does LoRA work mathematically?\nAnswer: LoRA represents weight updates as a product of two low-rank matrices: \u0394W = AB, where A is (d\u00d7r) and B is (r\u00d7d), with rank r << d. During inference, W' = W\u2080 + \u03b1AB, where W\u2080 is frozen pre-trained weights and \u03b1 is a scaling factor."}
{"text": "Q: What is unsupervised learning?\nA: Unsupervised learning discovers hidden patterns in unlabeled data without predefined categories. The algorithm explores data structure through techniques like clustering, dimensionality reduction, and density estimation, revealing natural groupings or relationships."}
{"text": "Question: Explain convolutional layers.\nAnswer: Convolutional layers apply learnable filters (kernels) to input feature maps through convolution operations. Each filter detects specific patterns (edges, textures, objects) and produces feature maps showing where those patterns occur spatially."}
{"text": "User: What is prompt tuning?\nAssistant: Prompt tuning optimizes continuous prompt embeddings prepended to inputs while keeping the entire model frozen. It's an extremely parameter-efficient method requiring only a small prompt matrix, effective for large models where even LoRA might be expensive."}
{"text": "Question: Explain the encoder-decoder architecture.\nAnswer: Encoder-decoder architectures consist of an encoder that processes input into representations and a decoder that generates output. They're used for sequence-to-sequence tasks like translation, where input and output lengths differ."}
{"text": "Question: What is prompt tuning?\nAnswer: Prompt tuning optimizes continuous prompt embeddings prepended to inputs while keeping the entire model frozen. It's an extremely parameter-efficient method requiring only a small prompt matrix, effective for large models where even LoRA might be expensive."}
{"text": "User: What is parameter-efficient fine-tuning?\nAssistant: Parameter-efficient fine-tuning adapts large pre-trained models while updating only a small fraction of parameters. Methods like LoRA, adapters, and prefix tuning achieve comparable performance to full fine-tuning with drastically reduced memory and computational requirements."}
{"text": "Question: What is instance segmentation?\nAnswer: Instance segmentation distinguishes between different instances of the same class, combining object detection and semantic segmentation. It assigns unique labels to each object instance, useful for counting or tracking individual objects."}
{"text": "User: What is a learning rate schedule?\nAssistant: Learning rate schedules adjust the learning rate during training, typically decreasing it over time. Common strategies include step decay, exponential decay, and cosine annealing. Proper scheduling improves convergence and final performance."}
{"text": "Explain the ReLU activation function.\n\nReLU (Rectified Linear Unit) outputs max(0, x), zero for negative inputs and identity for positive. It's computationally efficient, mitigates vanishing gradients, and induces sparsity. It's the most popular activation for hidden layers in deep networks."}
{"text": "User: What is overfitting?\nAssistant: Overfitting occurs when a model learns training data too well, including noise and outliers. The model memorizes rather than generalizes, achieving high training accuracy but poor performance on new data. Signs include large gap between train and validation performance."}
{"text": "What is word embedding?\n\nWord embeddings represent words as dense, low-dimensional vectors that capture semantic relationships. Words with similar meanings have similar vectors, enabling models to understand semantic similarity and perform arithmetic with meaning (king - man + woman \u2248 queen)."}
{"text": "Question: Explain the VGG architecture.\nAnswer: VGG uses very deep networks (16-19 layers) with small 3\u00d73 convolutional filters stacked repeatedly. This simple, uniform architecture demonstrates that network depth significantly impacts performance, though it's computationally expensive."}
{"text": "Question: What is a pooling layer?\nAnswer: Pooling layers downsample feature maps by combining neighboring values, typically using max or average operations. This reduces spatial dimensions, computational cost, and provides translation invariance while retaining important features."}
{"text": "How does LoRA work mathematically?\n\nLoRA represents weight updates as a product of two low-rank matrices: \u0394W = AB, where A is (d\u00d7r) and B is (r\u00d7d), with rank r << d. During inference, W' = W\u2080 + \u03b1AB, where W\u2080 is frozen pre-trained weights and \u03b1 is a scaling factor."}
{"text": "What is Bayesian optimization?\n\nBayesian optimization builds a probabilistic model of the objective function and uses it to select promising hyperparameters to evaluate next. It balances exploration and exploitation, efficiently finding good configurations with fewer expensive evaluations."}
{"text": "Q: What is a neural network?\nA: A neural network is a computational model inspired by biological neurons, consisting of interconnected nodes (artificial neurons) organized in layers. It learns to map inputs to outputs through adjustable weights that are optimized during training."}
{"text": "User: What is the Adam optimizer?\nAssistant: Adam (Adaptive Moment Estimation) combines momentum and adaptive learning rates per parameter. It maintains exponential moving averages of gradients and squared gradients, adapting learning rates based on first and second moment estimates for efficient optimization."}
{"text": "Q: What is the vanishing gradient problem?\nA: The vanishing gradient problem occurs when gradients become exponentially small during backpropagation through many layers. This happens with saturating activations (sigmoid/tanh) and prevents deep network training as early layers receive tiny updates."}
{"text": "Q: What is LoRA alpha?\nA: LoRA alpha (\u03b1) is a scaling hyperparameter that controls the magnitude of LoRA updates relative to original weights. The effective learning rate for LoRA parameters is scaled by \u03b1/r, where r is rank. Common practice sets \u03b1 = 2r."}
{"text": "Question: What are residual connections?\nAnswer: Residual (skip) connections add layer inputs to outputs, learning residual mappings F(x) instead of desired mappings H(x) = F(x) + x. This enables training very deep networks by providing gradient highways and making learning easier."}
{"text": "Q: Explain the curse of dimensionality.\nA: The curse of dimensionality refers to phenomena that arise when working with high-dimensional data. As dimensions increase, data becomes sparse, distances lose meaning, and exponentially more samples are needed to maintain density and statistical significance."}
{"text": "User: What is the exploding gradient problem?\nAssistant: The exploding gradient problem occurs when gradients grow exponentially large during backpropagation, causing numerical instability. It's common in RNNs with long sequences and can be mitigated with gradient clipping, careful initialization, or architecture changes."}
{"text": "Q: What is ROC curve?\nA: The ROC (Receiver Operating Characteristic) curve plots true positive rate (recall) against false positive rate across all classification thresholds. It visualizes the tradeoff between sensitivity and specificity independent of class distribution."}
{"text": "User: How does LoRA work mathematically?\nAssistant: LoRA represents weight updates as a product of two low-rank matrices: \u0394W = AB, where A is (d\u00d7r) and B is (r\u00d7d), with rank r << d. During inference, W' = W\u2080 + \u03b1AB, where W\u2080 is frozen pre-trained weights and \u03b1 is a scaling factor."}
{"text": "Q: What is recall?\nA: Recall (sensitivity) is the fraction of true positives among all actual positives: TP / (TP + FN). It answers 'Of all actual positive instances, how many did we find?' High recall means few false negatives."}
{"text": "Question: What is layer normalization?\nAnswer: Layer normalization normalizes across features for each sample independently, unlike batch norm which normalizes across the batch. It's useful for RNNs and transformers where batch sizes vary or batch statistics are unreliable."}
{"text": "What is gradient clipping?\n\nGradient clipping caps gradient magnitudes to a maximum value, preventing exploding gradients. It's essential for RNN training and stabilizes optimization when gradients occasionally spike. Common thresholds are clip by norm or clip by value."}
{"text": "Q: What is a learning rate schedule?\nA: Learning rate schedules adjust the learning rate during training, typically decreasing it over time. Common strategies include step decay, exponential decay, and cosine annealing. Proper scheduling improves convergence and final performance."}
{"text": "User: Explain stochastic gradient descent.\nAssistant: Stochastic gradient descent (SGD) updates parameters using gradients computed from single samples or small mini-batches rather than the entire dataset. This introduces noise but enables faster iterations, better generalization, and escape from shallow local minima."}
{"text": "User: What is semantic segmentation?\nAssistant: Semantic segmentation assigns a class label to every pixel in an image, creating dense predictions. It's used for tasks like medical image analysis, autonomous driving, and scene understanding where precise boundaries matter."}
{"text": "Question: Explain the difference between classification and regression.\nAnswer: Classification predicts discrete categories or labels (e.g., cat vs dog), while regression predicts continuous numerical values (e.g., temperature). Both are supervised learning tasks but differ in output type and evaluation metrics."}
{"text": "Explain cross-validation.\n\nCross-validation assesses model performance by partitioning data into k folds, iteratively training on k-1 folds and validating on the remaining fold. Results are averaged across all folds, providing robust performance estimates and reducing variance."}
{"text": "What is ROC curve?\n\nThe ROC (Receiver Operating Characteristic) curve plots true positive rate (recall) against false positive rate across all classification thresholds. It visualizes the tradeoff between sensitivity and specificity independent of class distribution."}
{"text": "What is the vanishing gradient problem?\n\nThe vanishing gradient problem occurs when gradients become exponentially small during backpropagation through many layers. This happens with saturating activations (sigmoid/tanh) and prevents deep network training as early layers receive tiny updates."}
{"text": "Question: What is gradient descent?\nAnswer: Gradient descent is an iterative optimization algorithm that adjusts parameters in the direction of steepest descent of the loss function. It uses gradients (derivatives) to determine update direction and magnitude, moving toward a local minimum."}
{"text": "Q: What is leave-one-out cross-validation?\nA: Leave-one-out cross-validation (LOOCV) is an extreme case of k-fold CV where k equals the dataset size. Each iteration uses one sample for validation and all others for training. It's computationally expensive but provides maximum training data per iteration."}
{"text": "Question: What is label smoothing?\nAnswer: Label smoothing replaces hard 0/1 targets with soft targets (e.g., 0.9/0.1), reducing model overconfidence. It acts as regularization, preventing overfitting and improving model calibration, especially important for classification with many classes."}
{"text": "Q: What is a multilayer perceptron?\nA: A multilayer perceptron (MLP) is a feedforward neural network with one or more hidden layers between input and output. Hidden layers with non-linear activations enable learning complex, non-linear relationships between inputs and outputs."}
{"text": "What is batch normalization?\n\nBatch normalization normalizes layer inputs across mini-batches to have zero mean and unit variance. This reduces internal covariate shift, stabilizes training, allows higher learning rates, and acts as a regularizer, significantly accelerating deep network training."}
{"text": "User: What is stacking?\nAssistant: Stacking (Stacked Generalization) trains a meta-model to combine predictions from multiple base models. The meta-model learns optimal weights or combinations of base predictions, often outperforming simple averaging."}
{"text": "User: What is soft prompting?\nAssistant: Soft prompting learns continuous prompt embeddings in the model's embedding space rather than discrete text prompts. Unlike hard prompts (actual text), soft prompts can occupy regions of embedding space not reachable by vocabulary tokens, providing more flexibility."}
{"text": "Q: What is max pooling?\nA: Max pooling selects the maximum value within each pooling window, retaining the strongest activation. It provides translation invariance, reduces dimensions, and helps the network focus on whether features are present rather than their exact location."}
{"text": "Question: Describe reinforcement learning.\nAnswer: Reinforcement learning trains agents to make sequential decisions by interacting with an environment. The agent receives rewards or penalties for actions and learns to maximize cumulative reward over time through trial and error."}
{"text": "Question: What is semantic segmentation?\nAnswer: Semantic segmentation assigns a class label to every pixel in an image, creating dense predictions. It's used for tasks like medical image analysis, autonomous driving, and scene understanding where precise boundaries matter."}
{"text": "Explain transfer learning.\n\nTransfer learning applies knowledge from models trained on one task to improve performance on different but related tasks. Pre-trained models capture useful features and representations that transfer across domains, enabling efficient learning with limited data."}
{"text": "Question: Explain the curse of dimensionality.\nAnswer: The curse of dimensionality refers to phenomena that arise when working with high-dimensional data. As dimensions increase, data becomes sparse, distances lose meaning, and exponentially more samples are needed to maintain density and statistical significance."}
{"text": "User: Explain the encoder-decoder architecture.\nAssistant: Encoder-decoder architectures consist of an encoder that processes input into representations and a decoder that generates output. They're used for sequence-to-sequence tasks like translation, where input and output lengths differ."}
{"text": "What is a learning rate schedule?\n\nLearning rate schedules adjust the learning rate during training, typically decreasing it over time. Common strategies include step decay, exponential decay, and cosine annealing. Proper scheduling improves convergence and final performance."}
{"text": "Q: What is AUC?\nA: AUC (Area Under the Curve) summarizes ROC curve performance in a single number from 0 to 1. AUC=0.5 means random guessing, AUC=1.0 means perfect classification. It measures discriminative ability across all possible thresholds."}
{"text": "Question: What is curriculum learning?\nAnswer: Curriculum learning trains models on progressively harder examples, starting with easy cases and gradually increasing difficulty. This mimics human learning, often improving convergence speed, final performance, and generalization compared to random ordering."}
{"text": "User: Explain stride in convolution.\nAssistant: Stride determines the step size when sliding filters across inputs. Stride 1 moves one pixel at a time; stride 2 skips every other position. Larger strides reduce output dimensions and computational cost but may miss fine details."}
{"text": "User: What is BERT?\nAssistant: BERT (Bidirectional Encoder Representations from Transformers) pre-trains bidirectional transformers on large corpora using masked language modeling and next sentence prediction. It captures deep bidirectional context and achieves state-of-the-art results on many NLP tasks."}
{"text": "Explain stochastic gradient descent.\n\nStochastic gradient descent (SGD) updates parameters using gradients computed from single samples or small mini-batches rather than the entire dataset. This introduces noise but enables faster iterations, better generalization, and escape from shallow local minima."}
{"text": "Question: Explain the perceptron.\nAnswer: The perceptron is the simplest neural network unit, computing a weighted sum of inputs plus a bias, then applying a step activation function. While limited to linear decision boundaries, it forms the foundation of modern neural networks."}
{"text": "User: Explain ensemble learning.\nAssistant: Ensemble learning combines multiple models to produce better predictions than any individual model. By aggregating diverse models' outputs, ensembles reduce variance, increase robustness, and often achieve superior performance."}
{"text": "User: What is dropout?\nAssistant: Dropout randomly deactivates neurons during training with probability p (typically 0.5). This prevents co-adaptation of features, creates an ensemble effect by training different sub-networks, and serves as powerful regularization for deep networks."}
{"text": "Q: Explain supervised learning.\nA: Supervised learning is a machine learning paradigm where models learn from labeled training data. The algorithm learns a mapping from inputs to outputs using examples, then applies this learned mapping to make predictions on new, unseen data. Common tasks include classification and regression."}
{"text": "Question: What is leave-one-out cross-validation?\nAnswer: Leave-one-out cross-validation (LOOCV) is an extreme case of k-fold CV where k equals the dataset size. Each iteration uses one sample for validation and all others for training. It's computationally expensive but provides maximum training data per iteration."}
{"text": "Q: What is self-attention?\nA: Self-attention computes attention within a single sequence, relating different positions to each other. Each element attends to all elements (including itself), learning which parts are relevant for representing each position in context."}
{"text": "User: What is next sentence prediction?\nAssistant: Next sentence prediction is a BERT pre-training task that determines if two sentences are consecutive in the original text. It helps the model learn sentence-level relationships useful for tasks like question answering and natural language inference."}
{"text": "User: What is catastrophic forgetting?\nAssistant: Catastrophic forgetting occurs when a neural network abruptly loses previously learned knowledge upon learning new information. It's a major challenge in continual learning, as updating weights for new tasks can overwrite weights important for old tasks."}
{"text": "What is fine-tuning in machine learning?\n\nFine-tuning adapts a pre-trained model to a specific task by continuing training on task-specific data. It leverages knowledge learned from large datasets, requiring less data and computation than training from scratch while achieving better performance."}
{"text": "Question: What is ResNet?\nAnswer: ResNet introduces skip connections (residual connections) that add layer inputs directly to outputs. This enables training very deep networks (50-152+ layers) by allowing gradients to flow directly through the network, solving degradation problems."}
{"text": "Question: What is an activation function?\nAnswer: An activation function introduces non-linearity into neural networks, enabling them to learn complex patterns. It determines whether and how strongly a neuron fires based on its input. Examples include ReLU, sigmoid, and tanh."}
{"text": "User: What is a neural network?\nAssistant: A neural network is a computational model inspired by biological neurons, consisting of interconnected nodes (artificial neurons) organized in layers. It learns to map inputs to outputs through adjustable weights that are optimized during training."}
{"text": "User: What is a hyperparameter?\nAssistant: Hyperparameters are configuration settings that control the learning process but aren't learned from data. Examples include learning rate, batch size, number of layers, and regularization strength. They must be set before training and tuned for optimal performance."}
{"text": "What is the rank in LoRA?\n\nThe rank (r) in LoRA determines the dimensionality of the low-rank decomposition. Lower rank means fewer parameters and faster training but less expressiveness. Higher rank provides more capacity but increases cost. Typical values range from 4 to 64."}
{"text": "Q: What is a convolutional neural network?\nA: A CNN is a neural network specialized for processing grid-structured data like images. It uses convolutional layers to detect local patterns through learnable filters, pooling layers to reduce spatial dimensions, and fully connected layers for classification."}
{"text": "Q: What is the exploding gradient problem?\nA: The exploding gradient problem occurs when gradients grow exponentially large during backpropagation, causing numerical instability. It's common in RNNs with long sequences and can be mitigated with gradient clipping, careful initialization, or architecture changes."}
{"text": "Explain the encoder-decoder architecture.\n\nEncoder-decoder architectures consist of an encoder that processes input into representations and a decoder that generates output. They're used for sequence-to-sequence tasks like translation, where input and output lengths differ."}
{"text": "User: What is an activation function?\nAssistant: An activation function introduces non-linearity into neural networks, enabling them to learn complex patterns. It determines whether and how strongly a neuron fires based on its input. Examples include ReLU, sigmoid, and tanh."}
{"text": "Can multiple LoRA adapters be used together?\n\nYes, multiple LoRA adapters can be combined for different tasks or aspects. They can be weighted and summed: W = W_base + \u03b1\u2081A\u2081B\u2081 + \u03b1\u2082A\u2082B\u2082. This enables multi-task models, mixing capabilities, or compositional task solving."}
{"text": "Question: What is parameter-efficient fine-tuning?\nAnswer: Parameter-efficient fine-tuning adapts large pre-trained models while updating only a small fraction of parameters. Methods like LoRA, adapters, and prefix tuning achieve comparable performance to full fine-tuning with drastically reduced memory and computational requirements."}
{"text": "Question: What is data augmentation in images?\nAnswer: Data augmentation artificially increases training data diversity by applying transformations like rotations, flips, crops, color jitter, and noise. This improves model robustness, reduces overfitting, and teaches invariance to common variations."}
{"text": "User: What is average pooling?\nAssistant: Average pooling computes the mean of values within each pooling window. It provides smoother downsampling than max pooling and is sometimes used in network final layers to aggregate spatial information."}
{"text": "Q: What is LoRA?\nA: LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning technique that decomposes weight updates into low-rank matrices. Instead of updating weight W directly, it learns W + AB where A and B are much smaller, reducing trainable parameters by orders of magnitude."}
{"text": "What is QLoRA?\n\nQLoRA (Quantized LoRA) combines LoRA with 4-bit quantization of base model weights. It enables fine-tuning of very large models on consumer GPUs by drastically reducing memory requirements while maintaining competitive performance through careful quantization and LoRA adaptation."}
{"text": "Q: What is the F1 score?\nA: The F1 score is the harmonic mean of precision and recall: 2 \u00d7 (precision \u00d7 recall) / (precision + recall). It balances both metrics, useful when you need a single score accounting for both false positives and false negatives."}
{"text": "Question: What is GRU?\nAnswer: Gated Recurrent Units (GRU) simplify LSTMs by combining forget and input gates into an update gate and merging cell and hidden states. They achieve similar performance to LSTMs with fewer parameters and faster training."}
{"text": "What is unsupervised learning?\n\nUnsupervised learning discovers hidden patterns in unlabeled data without predefined categories. The algorithm explores data structure through techniques like clustering, dimensionality reduction, and density estimation, revealing natural groupings or relationships."}
{"text": "User: Explain supervised learning.\nAssistant: Supervised learning is a machine learning paradigm where models learn from labeled training data. The algorithm learns a mapping from inputs to outputs using examples, then applies this learned mapping to make predictions on new, unseen data. Common tasks include classification and regression."}
{"text": "What is a pooling layer?\n\nPooling layers downsample feature maps by combining neighboring values, typically using max or average operations. This reduces spatial dimensions, computational cost, and provides translation invariance while retaining important features."}
{"text": "Q: What is GRU?\nA: Gated Recurrent Units (GRU) simplify LSTMs by combining forget and input gates into an update gate and merging cell and hidden states. They achieve similar performance to LSTMs with fewer parameters and faster training."}
{"text": "What is mean absolute error?\n\nMean absolute error (MAE) averages the absolute differences between predictions and targets. Unlike MSE, it treats all errors equally and is more robust to outliers, making it useful when large errors should not be overly penalized."}
{"text": "Question: What is BERT?\nAnswer: BERT (Bidirectional Encoder Representations from Transformers) pre-trains bidirectional transformers on large corpora using masked language modeling and next sentence prediction. It captures deep bidirectional context and achieves state-of-the-art results on many NLP tasks."}
{"text": "What is a convolution operation?\n\nConvolution slides a small filter (kernel) across an input, computing element-wise multiplication and summing results at each position. This creates a feature map indicating filter response strength across spatial locations, detecting local patterns."}
{"text": "User: What is word embedding?\nAssistant: Word embeddings represent words as dense, low-dimensional vectors that capture semantic relationships. Words with similar meanings have similar vectors, enabling models to understand semantic similarity and perform arithmetic with meaning (king - man + woman \u2248 queen)."}
{"text": "Q: What is the rank in LoRA?\nA: The rank (r) in LoRA determines the dimensionality of the low-rank decomposition. Lower rank means fewer parameters and faster training but less expressiveness. Higher rank provides more capacity but increases cost. Typical values range from 4 to 64."}
{"text": "User: What is curriculum learning?\nAssistant: Curriculum learning trains models on progressively harder examples, starting with easy cases and gradually increasing difficulty. This mimics human learning, often improving convergence speed, final performance, and generalization compared to random ordering."}
{"text": "Q: What is the softmax function?\nA: Softmax converts a vector of values into a probability distribution that sums to 1. It's used in multi-class classification output layers, exponentiating each element and normalizing by the sum."}
{"text": "Question: What is the rank in LoRA?\nAnswer: The rank (r) in LoRA determines the dimensionality of the low-rank decomposition. Lower rank means fewer parameters and faster training but less expressiveness. Higher rank provides more capacity but increases cost. Typical values range from 4 to 64."}
{"text": "What is parameter-efficient fine-tuning?\n\nParameter-efficient fine-tuning adapts large pre-trained models while updating only a small fraction of parameters. Methods like LoRA, adapters, and prefix tuning achieve comparable performance to full fine-tuning with drastically reduced memory and computational requirements."}
{"text": "Explain cross-entropy loss.\n\nCross-entropy loss measures the difference between predicted probability distributions and true labels. For classification, it's the negative log probability of the correct class. It's the standard loss for classification tasks."}
{"text": "Question: Can multiple LoRA adapters be used together?\nAnswer: Yes, multiple LoRA adapters can be combined for different tasks or aspects. They can be weighted and summed: W = W_base + \u03b1\u2081A\u2081B\u2081 + \u03b1\u2082A\u2082B\u2082. This enables multi-task models, mixing capabilities, or compositional task solving."}
{"text": "Question: Explain LSTM networks.\nAnswer: Long Short-Term Memory (LSTM) networks address RNN's vanishing gradient problem using gating mechanisms (input, forget, output gates) and memory cells. They can learn long-range dependencies by selectively remembering or forgetting information."}
{"text": "Q: What is weight initialization?\nA: Weight initialization sets initial parameter values before training. Poor initialization can cause vanishing/exploding gradients or slow convergence. Modern methods like Xavier and He initialization account for layer sizes and activations for stable training."}
{"text": "Q: Explain the VGG architecture.\nA: VGG uses very deep networks (16-19 layers) with small 3\u00d73 convolutional filters stacked repeatedly. This simple, uniform architecture demonstrates that network depth significantly impacts performance, though it's computationally expensive."}
{"text": "Q: What modules should be targeted with LoRA?\nA: LoRA typically targets attention weight matrices (Query, Key, Value, Output projections) as they contain most parameters and are most important for adaptation. Some implementations also target feed-forward network layers for additional capacity."}
{"text": "Q: What is perplexity?\nA: Perplexity measures how well a probability model predicts samples, commonly used for language models. It's the exponential of cross-entropy loss. Lower perplexity indicates better predictions, with perplexity of 1 meaning perfect prediction."}
{"text": "Question: What is mixed precision training?\nAnswer: Mixed precision training uses both FP16 (fast, memory-efficient) and FP32 (stable, precise) arithmetic. Activations and gradients use FP16 while master weights stay in FP32. This accelerates training and reduces memory with minimal quality loss."}
{"text": "User: What is hyperparameter tuning?\nAssistant: Hyperparameter tuning systematically searches for optimal hyperparameter values. Methods include grid search (exhaustive), random search (efficient), and Bayesian optimization (smart). Good tuning can dramatically improve model performance."}
{"text": "What is gradient descent?\n\nGradient descent is an iterative optimization algorithm that adjusts parameters in the direction of steepest descent of the loss function. It uses gradients (derivatives) to determine update direction and magnitude, moving toward a local minimum."}
{"text": "User: What is GloVe?\nAssistant: GloVe (Global Vectors) learns embeddings by factorizing word co-occurrence matrices. It captures both local context and global corpus statistics, combining benefits of matrix factorization and local context window methods like Word2Vec."}
{"text": "User: What is a training set?\nAssistant: A training set is the portion of data used to train a machine learning model. The model learns patterns, relationships, and parameters from this data. It's typically the largest split, often 60-80% of total data."}
{"text": "What is the learning rate?\n\nThe learning rate controls the step size in gradient descent optimization. Too high causes divergence or oscillation around minima; too low results in slow convergence or getting stuck. It's one of the most critical hyperparameters requiring careful tuning."}
{"text": "What is mini-batch gradient descent?\n\nMini-batch gradient descent computes gradients on small batches of samples (e.g., 32-512), balancing between full-batch stability and stochastic speed. It's the most common variant used in practice, offering efficient parallelization and stable convergence."}
{"text": "Question: What is bagging?\nAnswer: Bagging (Bootstrap Aggregating) trains multiple models on different random samples of training data (with replacement) and averages their predictions. It reduces variance and prevents overfitting, with Random Forests being a popular example."}
{"text": "What is a recurrent neural network?\n\nRNNs process sequential data by maintaining hidden states that carry information across time steps. Each step's output depends on current input and previous hidden state, enabling the network to capture temporal dependencies in sequences."}
{"text": "User: What is Bayesian optimization?\nAssistant: Bayesian optimization builds a probabilistic model of the objective function and uses it to select promising hyperparameters to evaluate next. It balances exploration and exploitation, efficiently finding good configurations with fewer expensive evaluations."}
{"text": "Q: Explain ensemble learning.\nA: Ensemble learning combines multiple models to produce better predictions than any individual model. By aggregating diverse models' outputs, ensembles reduce variance, increase robustness, and often achieve superior performance."}
{"text": "User: Explain the perceptron.\nAssistant: The perceptron is the simplest neural network unit, computing a weighted sum of inputs plus a bias, then applying a step activation function. While limited to linear decision boundaries, it forms the foundation of modern neural networks."}
{"text": "User: What is k-fold cross-validation?\nAssistant: K-fold cross-validation divides data into k equal parts (folds). The model trains k times, each time using a different fold for validation and the remaining k-1 for training. Common choices are k=5 or k=10."}
{"text": "What is layer normalization?\n\nLayer normalization normalizes across features for each sample independently, unlike batch norm which normalizes across the batch. It's useful for RNNs and transformers where batch sizes vary or batch statistics are unreliable."}
{"text": "Q: What is BLEU score?\nA: BLEU (Bilingual Evaluation Understudy) evaluates machine translation quality by comparing n-gram overlap between generated and reference translations. It ranges from 0 to 100, with higher scores indicating better translation quality."}
{"text": "Q: What is momentum in optimization?\nA: Momentum accelerates gradient descent by accumulating a velocity vector in directions of persistent gradient reduction. It helps overcome local minima, dampens oscillations, and speeds convergence, especially in ravines or plateaus."}
{"text": "What is perplexity?\n\nPerplexity measures how well a probability model predicts samples, commonly used for language models. It's the exponential of cross-entropy loss. Lower perplexity indicates better predictions, with perplexity of 1 meaning perfect prediction."}
{"text": "Question: Explain cross-entropy loss.\nAnswer: Cross-entropy loss measures the difference between predicted probability distributions and true labels. For classification, it's the negative log probability of the correct class. It's the standard loss for classification tasks."}
{"text": "User: What is the attention mechanism?\nAssistant: Attention allows models to focus on relevant input parts when processing each output. It computes weighted combinations of inputs based on learned relevance scores, enabling models to handle variable-length sequences and capture long-range dependencies effectively."}
{"text": "User: What are residual connections?\nAssistant: Residual (skip) connections add layer inputs to outputs, learning residual mappings F(x) instead of desired mappings H(x) = F(x) + x. This enables training very deep networks by providing gradient highways and making learning easier."}
{"text": "What is the exploding gradient problem?\n\nThe exploding gradient problem occurs when gradients grow exponentially large during backpropagation, causing numerical instability. It's common in RNNs with long sequences and can be mitigated with gradient clipping, careful initialization, or architecture changes."}
{"text": "User: What is AUC?\nAssistant: AUC (Area Under the Curve) summarizes ROC curve performance in a single number from 0 to 1. AUC=0.5 means random guessing, AUC=1.0 means perfect classification. It measures discriminative ability across all possible thresholds."}
{"text": "Q: What is named entity recognition?\nA: Named Entity Recognition (NER) identifies and classifies named entities (people, organizations, locations, dates) in text. It's a fundamental NLP task used in information extraction, question answering, and knowledge graph construction."}
{"text": "User: What is the learning rate?\nAssistant: The learning rate controls the step size in gradient descent optimization. Too high causes divergence or oscillation around minima; too low results in slow convergence or getting stuck. It's one of the most critical hyperparameters requiring careful tuning."}
{"text": "User: What is machine learning?\nAssistant: Machine learning is a branch of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed. It uses algorithms to identify patterns in data and make predictions or decisions based on those patterns."}
{"text": "User: What is the bias-variance tradeoff?\nAssistant: The bias-variance tradeoff describes the balance between a model's ability to fit training data (low bias) and generalize to new data (low variance). High bias leads to underfitting, high variance to overfitting. Optimal models balance both."}
{"text": "Q: What is the Adam optimizer?\nA: Adam (Adaptive Moment Estimation) combines momentum and adaptive learning rates per parameter. It maintains exponential moving averages of gradients and squared gradients, adapting learning rates based on first and second moment estimates for efficient optimization."}
{"text": "User: Explain adapter layers.\nAssistant: Adapter layers are small bottleneck modules inserted into pre-trained models. They consist of down-projection, non-linearity, and up-projection layers. Only adapter parameters are trained while original weights stay frozen, enabling efficient multi-task learning."}
{"text": "Q: What is bagging?\nA: Bagging (Bootstrap Aggregating) trains multiple models on different random samples of training data (with replacement) and averages their predictions. It reduces variance and prevents overfitting, with Random Forests being a popular example."}
{"text": "User: What are the advantages of LoRA over full fine-tuning?\nAssistant: LoRA requires 10-100x fewer trainable parameters, drastically reducing memory usage and training time. It enables fine-tuning large models on consumer hardware, allows storing multiple task-specific adapters cheaply, and provides implicit regularization against overfitting."}
{"text": "Q: What is machine translation?\nA: Machine translation automatically translates text from one language to another. Modern neural machine translation uses encoder-decoder architectures with attention, achieving near-human performance for some language pairs."}
{"text": "User: What is instance segmentation?\nAssistant: Instance segmentation distinguishes between different instances of the same class, combining object detection and semantic segmentation. It assigns unique labels to each object instance, useful for counting or tracking individual objects."}
{"text": "Question: What is precision?\nAnswer: Precision is the fraction of true positives among all positive predictions: TP / (TP + FP). It answers 'Of all instances predicted positive, how many were actually positive?' High precision means few false positives."}
{"text": "What is the Adam optimizer?\n\nAdam (Adaptive Moment Estimation) combines momentum and adaptive learning rates per parameter. It maintains exponential moving averages of gradients and squared gradients, adapting learning rates based on first and second moment estimates for efficient optimization."}
{"text": "What is accuracy?\n\nAccuracy is the fraction of correct predictions: (TP + TN) / Total. While intuitive, it's misleading for imbalanced datasets where a naive baseline achieves high accuracy by always predicting the majority class."}
{"text": "User: What is stratified cross-validation?\nAssistant: Stratified cross-validation ensures each fold has approximately the same class distribution as the original dataset. This is crucial for imbalanced datasets to ensure representative training and validation sets in each fold."}
{"text": "What is mixed precision training?\n\nMixed precision training uses both FP16 (fast, memory-efficient) and FP32 (stable, precise) arithmetic. Activations and gradients use FP16 while master weights stay in FP32. This accelerates training and reduces memory with minimal quality loss."}
{"text": "Question: What is mean squared error?\nAnswer: Mean squared error (MSE) averages the squared differences between predictions and targets. It's the standard loss function for regression, penalizing large errors more heavily than small ones due to squaring."}
{"text": "Q: Explain stochastic gradient descent.\nA: Stochastic gradient descent (SGD) updates parameters using gradients computed from single samples or small mini-batches rather than the entire dataset. This introduces noise but enables faster iterations, better generalization, and escape from shallow local minima."}
{"text": "User: What is Xavier initialization?\nAssistant: Xavier (Glorot) initialization sets weights from a distribution with variance 1/n_in (or 2/(n_in+n_out)), where n_in and n_out are input and output dimensions. It maintains activation and gradient variance across layers for stable training with tanh/sigmoid."}
{"text": "User: What is random search?\nAssistant: Random search samples hyperparameter combinations randomly from specified distributions. It's more efficient than grid search, especially when some hyperparameters don't significantly affect performance, and can discover better configurations with fewer evaluations."}
{"text": "User: What is the confusion matrix?\nAssistant: A confusion matrix tabulates classification results showing true positives, false positives, true negatives, and false negatives. It provides detailed insight into model performance, revealing which classes are confused and error patterns."}
{"text": "User: What is momentum in optimization?\nAssistant: Momentum accelerates gradient descent by accumulating a velocity vector in directions of persistent gradient reduction. It helps overcome local minima, dampens oscillations, and speeds convergence, especially in ravines or plateaus."}
{"text": "User: Explain LSTM networks.\nAssistant: Long Short-Term Memory (LSTM) networks address RNN's vanishing gradient problem using gating mechanisms (input, forget, output gates) and memory cells. They can learn long-range dependencies by selectively remembering or forgetting information."}
{"text": "Q: What is dropout?\nA: Dropout randomly deactivates neurons during training with probability p (typically 0.5). This prevents co-adaptation of features, creates an ensemble effect by training different sub-networks, and serves as powerful regularization for deep networks."}
{"text": "User: What is a convolutional neural network?\nAssistant: A CNN is a neural network specialized for processing grid-structured data like images. It uses convolutional layers to detect local patterns through learnable filters, pooling layers to reduce spatial dimensions, and fully connected layers for classification."}
{"text": "Explain the VGG architecture.\n\nVGG uses very deep networks (16-19 layers) with small 3\u00d73 convolutional filters stacked repeatedly. This simple, uniform architecture demonstrates that network depth significantly impacts performance, though it's computationally expensive."}
{"text": "Question: Explain stochastic gradient descent.\nAnswer: Stochastic gradient descent (SGD) updates parameters using gradients computed from single samples or small mini-batches rather than the entire dataset. This introduces noise but enables faster iterations, better generalization, and escape from shallow local minima."}
{"text": "Q: What is a recurrent neural network?\nA: RNNs process sequential data by maintaining hidden states that carry information across time steps. Each step's output depends on current input and previous hidden state, enabling the network to capture temporal dependencies in sequences."}
{"text": "What is backpropagation?\n\nBackpropagation is the algorithm for training neural networks. It computes gradients of the loss function with respect to each weight by propagating errors backward through the network using the chain rule, enabling gradient descent optimization."}
{"text": "Question: What is ROC curve?\nAnswer: The ROC (Receiver Operating Characteristic) curve plots true positive rate (recall) against false positive rate across all classification thresholds. It visualizes the tradeoff between sensitivity and specificity independent of class distribution."}
{"text": "Q: What is precision?\nA: Precision is the fraction of true positives among all positive predictions: TP / (TP + FP). It answers 'Of all instances predicted positive, how many were actually positive?' High precision means few false positives."}
{"text": "What is instance segmentation?\n\nInstance segmentation distinguishes between different instances of the same class, combining object detection and semantic segmentation. It assigns unique labels to each object instance, useful for counting or tracking individual objects."}
{"text": "Q: What is question answering?\nA: Question answering systems take a question and context (or knowledge base) and generate or extract an answer. It combines reading comprehension, information retrieval, and natural language understanding."}
{"text": "Question: What is the bias-variance tradeoff?\nAnswer: The bias-variance tradeoff describes the balance between a model's ability to fit training data (low bias) and generalize to new data (low variance). High bias leads to underfitting, high variance to overfitting. Optimal models balance both."}
{"text": "User: What is precision?\nAssistant: Precision is the fraction of true positives among all positive predictions: TP / (TP + FP). It answers 'Of all instances predicted positive, how many were actually positive?' High precision means few false positives."}
{"text": "Q: Explain the difference between classification and regression.\nA: Classification predicts discrete categories or labels (e.g., cat vs dog), while regression predicts continuous numerical values (e.g., temperature). Both are supervised learning tasks but differ in output type and evaluation metrics."}
{"text": "User: Explain cross-entropy loss.\nAssistant: Cross-entropy loss measures the difference between predicted probability distributions and true labels. For classification, it's the negative log probability of the correct class. It's the standard loss for classification tasks."}
{"text": "Q: What is the attention mechanism?\nA: Attention allows models to focus on relevant input parts when processing each output. It computes weighted combinations of inputs based on learned relevance scores, enabling models to handle variable-length sequences and capture long-range dependencies effectively."}
{"text": "User: What is GPT?\nAssistant: GPT (Generative Pre-trained Transformer) is an autoregressive language model that predicts next tokens given previous context. It's pre-trained on large text corpora and can be fine-tuned for various tasks or used for few-shot learning."}
{"text": "Q: Explain the encoder-decoder architecture.\nA: Encoder-decoder architectures consist of an encoder that processes input into representations and a decoder that generates output. They're used for sequence-to-sequence tasks like translation, where input and output lengths differ."}
{"text": "User: What is LoRA?\nAssistant: LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning technique that decomposes weight updates into low-rank matrices. Instead of updating weight W directly, it learns W + AB where A and B are much smaller, reducing trainable parameters by orders of magnitude."}
{"text": "User: What is masked language modeling?\nAssistant: Masked language modeling randomly masks tokens in text and trains models to predict masked tokens from context. BERT uses this self-supervised objective to learn bidirectional representations from unlabeled text at scale."}
{"text": "Question: What is mini-batch gradient descent?\nAnswer: Mini-batch gradient descent computes gradients on small batches of samples (e.g., 32-512), balancing between full-batch stability and stochastic speed. It's the most common variant used in practice, offering efficient parallelization and stable convergence."}
{"text": "Question: What is transfer learning in computer vision?\nAnswer: Transfer learning uses models pre-trained on large datasets (like ImageNet) as starting points for new tasks. The network has already learned useful features, requiring less data and training time for downstream tasks through fine-tuning."}
{"text": "What is a convolutional neural network?\n\nA CNN is a neural network specialized for processing grid-structured data like images. It uses convolutional layers to detect local patterns through learnable filters, pooling layers to reduce spatial dimensions, and fully connected layers for classification."}
{"text": "Question: Explain transfer learning.\nAnswer: Transfer learning applies knowledge from models trained on one task to improve performance on different but related tasks. Pre-trained models capture useful features and representations that transfer across domains, enabling efficient learning with limited data."}
{"text": "What is the attention mechanism?\n\nAttention allows models to focus on relevant input parts when processing each output. It computes weighted combinations of inputs based on learned relevance scores, enabling models to handle variable-length sequences and capture long-range dependencies effectively."}
{"text": "User: Explain the VGG architecture.\nAssistant: VGG uses very deep networks (16-19 layers) with small 3\u00d73 convolutional filters stacked repeatedly. This simple, uniform architecture demonstrates that network depth significantly impacts performance, though it's computationally expensive."}
{"text": "What is weight decay?\n\nWeight decay adds a penalty proportional to weight magnitudes to the loss function, encouraging smaller weights. It's equivalent to L2 regularization and prevents overfitting by discouraging complex models. Typical values are 0.0001 to 0.01."}
{"text": "Q: What is Xavier initialization?\nA: Xavier (Glorot) initialization sets weights from a distribution with variance 1/n_in (or 2/(n_in+n_out)), where n_in and n_out are input and output dimensions. It maintains activation and gradient variance across layers for stable training with tanh/sigmoid."}
{"text": "Question: What is cosine annealing?\nAnswer: Cosine annealing decreases learning rate following a cosine curve from initial to minimum value over a period. It provides smooth decay and can be restarted periodically (warm restarts) to escape local minima and improve optimization."}
{"text": "User: Describe reinforcement learning.\nAssistant: Reinforcement learning trains agents to make sequential decisions by interacting with an environment. The agent receives rewards or penalties for actions and learns to maximize cumulative reward over time through trial and error."}
{"text": "User: What is object detection?\nAssistant: Object detection identifies and localizes multiple objects in images, predicting both class labels and bounding boxes. It's more challenging than classification as it requires determining what objects exist and where they are located."}
{"text": "Explain ensemble learning.\n\nEnsemble learning combines multiple models to produce better predictions than any individual model. By aggregating diverse models' outputs, ensembles reduce variance, increase robustness, and often achieve superior performance."}
{"text": "Question: What is machine translation?\nAnswer: Machine translation automatically translates text from one language to another. Modern neural machine translation uses encoder-decoder architectures with attention, achieving near-human performance for some language pairs."}
{"text": "What is overfitting?\n\nOverfitting occurs when a model learns training data too well, including noise and outliers. The model memorizes rather than generalizes, achieving high training accuracy but poor performance on new data. Signs include large gap between train and validation performance."}
{"text": "User: What is a filter/kernel?\nAssistant: A filter (or kernel) is a small matrix of learnable weights that slides across inputs in convolutional layers. It detects specific patterns through element-wise multiplication and summation, producing high responses where patterns match."}
{"text": "What is curriculum learning?\n\nCurriculum learning trains models on progressively harder examples, starting with easy cases and gradually increasing difficulty. This mimics human learning, often improving convergence speed, final performance, and generalization compared to random ordering."}
{"text": "Q: What is GloVe?\nA: GloVe (Global Vectors) learns embeddings by factorizing word co-occurrence matrices. It captures both local context and global corpus statistics, combining benefits of matrix factorization and local context window methods like Word2Vec."}
{"text": "Q: Explain Word2Vec.\nA: Word2Vec learns word embeddings by predicting words from context (CBOW) or context from words (Skip-gram). It trains a shallow neural network on large corpora, producing embeddings where semantically similar words cluster together in vector space."}
{"text": "Q: What is prompt tuning?\nA: Prompt tuning optimizes continuous prompt embeddings prepended to inputs while keeping the entire model frozen. It's an extremely parameter-efficient method requiring only a small prompt matrix, effective for large models where even LoRA might be expensive."}
{"text": "What is weight initialization?\n\nWeight initialization sets initial parameter values before training. Poor initialization can cause vanishing/exploding gradients or slow convergence. Modern methods like Xavier and He initialization account for layer sizes and activations for stable training."}
{"text": "User: What is layer normalization?\nAssistant: Layer normalization normalizes across features for each sample independently, unlike batch norm which normalizes across the batch. It's useful for RNNs and transformers where batch sizes vary or batch statistics are unreliable."}
{"text": "What is ResNet?\n\nResNet introduces skip connections (residual connections) that add layer inputs directly to outputs. This enables training very deep networks (50-152+ layers) by allowing gradients to flow directly through the network, solving degradation problems."}
{"text": "What is prefix tuning?\n\nPrefix tuning prepends learnable continuous vectors (virtual tokens) to input sequences at each layer. These prefix parameters are optimized while the entire pre-trained model stays frozen, guiding model behavior for specific tasks with minimal parameters."}
{"text": "What is cosine annealing?\n\nCosine annealing decreases learning rate following a cosine curve from initial to minimum value over a period. It provides smooth decay and can be restarted periodically (warm restarts) to escape local minima and improve optimization."}
{"text": "What is max pooling?\n\nMax pooling selects the maximum value within each pooling window, retaining the strongest activation. It provides translation invariance, reduces dimensions, and helps the network focus on whether features are present rather than their exact location."}
{"text": "Q: Explain convolutional layers.\nA: Convolutional layers apply learnable filters (kernels) to input feature maps through convolution operations. Each filter detects specific patterns (edges, textures, objects) and produces feature maps showing where those patterns occur spatially."}
{"text": "Question: What is word embedding?\nAnswer: Word embeddings represent words as dense, low-dimensional vectors that capture semantic relationships. Words with similar meanings have similar vectors, enabling models to understand semantic similarity and perform arithmetic with meaning (king - man + woman \u2248 queen)."}
{"text": "What is a multilayer perceptron?\n\nA multilayer perceptron (MLP) is a feedforward neural network with one or more hidden layers between input and output. Hidden layers with non-linear activations enable learning complex, non-linear relationships between inputs and outputs."}
{"text": "User: What is named entity recognition?\nAssistant: Named Entity Recognition (NER) identifies and classifies named entities (people, organizations, locations, dates) in text. It's a fundamental NLP task used in information extraction, question answering, and knowledge graph construction."}
{"text": "What is prompt tuning?\n\nPrompt tuning optimizes continuous prompt embeddings prepended to inputs while keeping the entire model frozen. It's an extremely parameter-efficient method requiring only a small prompt matrix, effective for large models where even LoRA might be expensive."}
{"text": "What is BLEU score?\n\nBLEU (Bilingual Evaluation Understudy) evaluates machine translation quality by comparing n-gram overlap between generated and reference translations. It ranges from 0 to 100, with higher scores indicating better translation quality."}
{"text": "Describe reinforcement learning.\n\nReinforcement learning trains agents to make sequential decisions by interacting with an environment. The agent receives rewards or penalties for actions and learns to maximize cumulative reward over time through trial and error."}
{"text": "Question: What is named entity recognition?\nAnswer: Named Entity Recognition (NER) identifies and classifies named entities (people, organizations, locations, dates) in text. It's a fundamental NLP task used in information extraction, question answering, and knowledge graph construction."}
{"text": "Question: What are feature maps?\nAnswer: Feature maps are the outputs of convolutional or pooling layers, representing the presence and strength of learned features at different spatial locations. Early layers detect simple features (edges), while deeper layers detect complex patterns (objects)."}
{"text": "User: What is question answering?\nAssistant: Question answering systems take a question and context (or knowledge base) and generate or extract an answer. It combines reading comprehension, information retrieval, and natural language understanding."}
{"text": "Q: What is masked language modeling?\nA: Masked language modeling randomly masks tokens in text and trains models to predict masked tokens from context. BERT uses this self-supervised objective to learn bidirectional representations from unlabeled text at scale."}
{"text": "What is learning rate warmup?\n\nLearning rate warmup gradually increases the learning rate from zero to the target value over initial training steps. This prevents destabilization from large gradients early in training when the model is far from optimal, especially important for large batch sizes."}
{"text": "Question: What is BLEU score?\nAnswer: BLEU (Bilingual Evaluation Understudy) evaluates machine translation quality by comparing n-gram overlap between generated and reference translations. It ranges from 0 to 100, with higher scores indicating better translation quality."}
{"text": "User: What is underfitting?\nAssistant: Underfitting happens when a model is too simple to capture underlying data patterns. It performs poorly on both training and test data because it lacks the capacity to learn the true relationship between inputs and outputs."}
{"text": "Q: What is k-fold cross-validation?\nA: K-fold cross-validation divides data into k equal parts (folds). The model trains k times, each time using a different fold for validation and the remaining k-1 for training. Common choices are k=5 or k=10."}
{"text": "Question: What is average pooling?\nAnswer: Average pooling computes the mean of values within each pooling window. It provides smoother downsampling than max pooling and is sometimes used in network final layers to aggregate spatial information."}
{"text": "Q: What is He initialization?\nA: He initialization uses variance 2/n_in, doubling Xavier's variance. It's designed for ReLU activations which kill half the gradients. This compensation maintains stable gradient flow through deep ReLU networks."}
{"text": "Question: What is learning rate warmup?\nAnswer: Learning rate warmup gradually increases the learning rate from zero to the target value over initial training steps. This prevents destabilization from large gradients early in training when the model is far from optimal, especially important for large batch sizes."}
{"text": "What is GPT?\n\nGPT (Generative Pre-trained Transformer) is an autoregressive language model that predicts next tokens given previous context. It's pre-trained on large text corpora and can be fine-tuned for various tasks or used for few-shot learning."}
{"text": "User: What is max pooling?\nAssistant: Max pooling selects the maximum value within each pooling window, retaining the strongest activation. It provides translation invariance, reduces dimensions, and helps the network focus on whether features are present rather than their exact location."}
{"text": "User: What is the intrinsic dimensionality hypothesis?\nAssistant: The intrinsic dimensionality hypothesis posits that successful model updates lie in a much lower-dimensional subspace than the full parameter space. This explains why low-rank methods like LoRA work well: the essential adaptations fit in small subspaces."}
{"text": "Q: Can multiple LoRA adapters be used together?\nA: Yes, multiple LoRA adapters can be combined for different tasks or aspects. They can be weighted and summed: W = W_base + \u03b1\u2081A\u2081B\u2081 + \u03b1\u2082A\u2082B\u2082. This enables multi-task models, mixing capabilities, or compositional task solving."}
{"text": "Q: Describe reinforcement learning.\nA: Reinforcement learning trains agents to make sequential decisions by interacting with an environment. The agent receives rewards or penalties for actions and learns to maximize cumulative reward over time through trial and error."}
{"text": "User: What is mean average precision?\nAssistant: Mean Average Precision (mAP) evaluates object detection and information retrieval by averaging precision at different recall levels. It accounts for both detection accuracy and localization quality, providing a comprehensive performance metric."}
{"text": "Q: What is gradient descent?\nA: Gradient descent is an iterative optimization algorithm that adjusts parameters in the direction of steepest descent of the loss function. It uses gradients (derivatives) to determine update direction and magnitude, moving toward a local minimum."}
{"text": "Question: What is QLoRA?\nAnswer: QLoRA (Quantized LoRA) combines LoRA with 4-bit quantization of base model weights. It enables fine-tuning of very large models on consumer GPUs by drastically reducing memory requirements while maintaining competitive performance through careful quantization and LoRA adaptation."}
{"text": "Q: What is natural language processing?\nA: Natural language processing (NLP) enables computers to understand, interpret, and generate human language. It combines linguistics, computer science, and machine learning to process text and speech for tasks like translation, sentiment analysis, and question answering."}
{"text": "What is LoRA alpha?\n\nLoRA alpha (\u03b1) is a scaling hyperparameter that controls the magnitude of LoRA updates relative to original weights. The effective learning rate for LoRA parameters is scaled by \u03b1/r, where r is rank. Common practice sets \u03b1 = 2r."}
{"text": "Question: What is the sigmoid activation function?\nAnswer: The sigmoid function maps inputs to (0,1) using the formula 1/(1+e^-x). It's useful for binary classification output layers and was historically popular for hidden layers, though it suffers from vanishing gradients."}
{"text": "What is the bias-variance tradeoff?\n\nThe bias-variance tradeoff describes the balance between a model's ability to fit training data (low bias) and generalize to new data (low variance). High bias leads to underfitting, high variance to overfitting. Optimal models balance both."}
{"text": "Q: What is a hyperparameter?\nA: Hyperparameters are configuration settings that control the learning process but aren't learned from data. Examples include learning rate, batch size, number of layers, and regularization strength. They must be set before training and tuned for optimal performance."}
{"text": "What is AUC?\n\nAUC (Area Under the Curve) summarizes ROC curve performance in a single number from 0 to 1. AUC=0.5 means random guessing, AUC=1.0 means perfect classification. It measures discriminative ability across all possible thresholds."}
{"text": "Question: What is weight decay?\nAnswer: Weight decay adds a penalty proportional to weight magnitudes to the loss function, encouraging smaller weights. It's equivalent to L2 regularization and prevents overfitting by discouraging complex models. Typical values are 0.0001 to 0.01."}
{"text": "User: What is the tanh activation function?\nAssistant: The tanh (hyperbolic tangent) function maps inputs to (-1,1). It's zero-centered, making optimization easier than sigmoid, but also suffers from vanishing gradients. It's sometimes used in RNNs and specific architectures."}
{"text": "User: How do you merge LoRA weights?\nAssistant: LoRA weights can be merged into base model weights after training: W_merged = W_base + \u03b1 \u00d7 A \u00d7 B. This eliminates inference overhead from adapter modules while preserving task-specific adaptations in a single model that can be deployed normally."}
{"text": "User: What is cosine annealing?\nAssistant: Cosine annealing decreases learning rate following a cosine curve from initial to minimum value over a period. It provides smooth decay and can be restarted periodically (warm restarts) to escape local minima and improve optimization."}
{"text": "Q: What is the transformer architecture?\nA: Transformers process sequences using self-attention mechanisms instead of recurrence. They enable parallel computation, capture long-range dependencies efficiently, and form the foundation of modern NLP models like BERT, GPT, and T5."}
{"text": "What is BERT?\n\nBERT (Bidirectional Encoder Representations from Transformers) pre-trains bidirectional transformers on large corpora using masked language modeling and next sentence prediction. It captures deep bidirectional context and achieves state-of-the-art results on many NLP tasks."}
{"text": "What is mean average precision?\n\nMean Average Precision (mAP) evaluates object detection and information retrieval by averaging precision at different recall levels. It accounts for both detection accuracy and localization quality, providing a comprehensive performance metric."}
{"text": "What are the advantages of LoRA over full fine-tuning?\n\nLoRA requires 10-100x fewer trainable parameters, drastically reducing memory usage and training time. It enables fine-tuning large models on consumer hardware, allows storing multiple task-specific adapters cheaply, and provides implicit regularization against overfitting."}
{"text": "Q: What is soft prompting?\nA: Soft prompting learns continuous prompt embeddings in the model's embedding space rather than discrete text prompts. Unlike hard prompts (actual text), soft prompts can occupy regions of embedding space not reachable by vocabulary tokens, providing more flexibility."}
{"text": "Q: What is gradient clipping?\nA: Gradient clipping caps gradient magnitudes to a maximum value, preventing exploding gradients. It's essential for RNN training and stabilizes optimization when gradients occasionally spike. Common thresholds are clip by norm or clip by value."}
{"text": "What is a loss function?\n\nA loss function (or cost function) quantifies the difference between model predictions and true labels. It provides a differentiable objective that optimization algorithms minimize during training to improve model performance."}
{"text": "What is feature selection?\n\nFeature selection identifies and retains only the most relevant features for a task, removing irrelevant or redundant ones. This reduces dimensionality, improves model interpretability, decreases training time, and can prevent overfitting."}
{"text": "Question: What is gradient clipping?\nAnswer: Gradient clipping caps gradient magnitudes to a maximum value, preventing exploding gradients. It's essential for RNN training and stabilizes optimization when gradients occasionally spike. Common thresholds are clip by norm or clip by value."}
{"text": "User: What is a convolution operation?\nAssistant: Convolution slides a small filter (kernel) across an input, computing element-wise multiplication and summing results at each position. This creates a feature map indicating filter response strength across spatial locations, detecting local patterns."}
{"text": "What is a validation set?\n\nA validation set is data held out from training to tune hyperparameters and make model selection decisions. It provides unbiased evaluation during development without touching the test set, preventing overfitting to test data."}
{"text": "User: What is feature engineering?\nAssistant: Feature engineering is the process of creating new features or transforming existing ones to improve model performance. It involves domain knowledge to extract meaningful representations that make patterns more apparent to learning algorithms."}
{"text": "Question: What is AUC?\nAnswer: AUC (Area Under the Curve) summarizes ROC curve performance in a single number from 0 to 1. AUC=0.5 means random guessing, AUC=1.0 means perfect classification. It measures discriminative ability across all possible thresholds."}
{"text": "Q: How does LoRA work mathematically?\nA: LoRA represents weight updates as a product of two low-rank matrices: \u0394W = AB, where A is (d\u00d7r) and B is (r\u00d7d), with rank r << d. During inference, W' = W\u2080 + \u03b1AB, where W\u2080 is frozen pre-trained weights and \u03b1 is a scaling factor."}
{"text": "User: What is a pooling layer?\nAssistant: Pooling layers downsample feature maps by combining neighboring values, typically using max or average operations. This reduces spatial dimensions, computational cost, and provides translation invariance while retaining important features."}
{"text": "Q: What is curriculum learning?\nA: Curriculum learning trains models on progressively harder examples, starting with easy cases and gradually increasing difficulty. This mimics human learning, often improving convergence speed, final performance, and generalization compared to random ordering."}
{"text": "Q: What is gradient accumulation?\nA: Gradient accumulation computes gradients over multiple mini-batches before updating weights. This simulates larger batch sizes without increased memory, as gradients are summed across batches then averaged for the update. Useful for memory-constrained training."}
{"text": "Q: What are feature maps?\nA: Feature maps are the outputs of convolutional or pooling layers, representing the presence and strength of learned features at different spatial locations. Early layers detect simple features (edges), while deeper layers detect complex patterns (objects)."}
{"text": "Q: What is accuracy?\nA: Accuracy is the fraction of correct predictions: (TP + TN) / Total. While intuitive, it's misleading for imbalanced datasets where a naive baseline achieves high accuracy by always predicting the majority class."}
{"text": "Q: What is feature engineering?\nA: Feature engineering is the process of creating new features or transforming existing ones to improve model performance. It involves domain knowledge to extract meaningful representations that make patterns more apparent to learning algorithms."}
{"text": "Question: What is a multilayer perceptron?\nAnswer: A multilayer perceptron (MLP) is a feedforward neural network with one or more hidden layers between input and output. Hidden layers with non-linear activations enable learning complex, non-linear relationships between inputs and outputs."}
{"text": "Q: What is a pooling layer?\nA: Pooling layers downsample feature maps by combining neighboring values, typically using max or average operations. This reduces spatial dimensions, computational cost, and provides translation invariance while retaining important features."}
{"text": "How do you merge LoRA weights?\n\nLoRA weights can be merged into base model weights after training: W_merged = W_base + \u03b1 \u00d7 A \u00d7 B. This eliminates inference overhead from adapter modules while preserving task-specific adaptations in a single model that can be deployed normally."}
{"text": "Q: What is a loss function?\nA: A loss function (or cost function) quantifies the difference between model predictions and true labels. It provides a differentiable objective that optimization algorithms minimize during training to improve model performance."}
{"text": "User: What is data augmentation in images?\nAssistant: Data augmentation artificially increases training data diversity by applying transformations like rotations, flips, crops, color jitter, and noise. This improves model robustness, reduces overfitting, and teaches invariance to common variations."}
{"text": "What is stratified cross-validation?\n\nStratified cross-validation ensures each fold has approximately the same class distribution as the original dataset. This is crucial for imbalanced datasets to ensure representative training and validation sets in each fold."}
{"text": "What is self-attention?\n\nSelf-attention computes attention within a single sequence, relating different positions to each other. Each element attends to all elements (including itself), learning which parts are relevant for representing each position in context."}
{"text": "Question: Explain the ReLU activation function.\nAnswer: ReLU (Rectified Linear Unit) outputs max(0, x), zero for negative inputs and identity for positive. It's computationally efficient, mitigates vanishing gradients, and induces sparsity. It's the most popular activation for hidden layers in deep networks."}
{"text": "Question: What is question answering?\nAnswer: Question answering systems take a question and context (or knowledge base) and generate or extract an answer. It combines reading comprehension, information retrieval, and natural language understanding."}
{"text": "User: What is early stopping?\nAssistant: Early stopping halts training when validation performance stops improving for a specified number of epochs (patience). It prevents overfitting by finding the point where the model generalizes best before it starts memorizing training data."}
{"text": "Question: What is the softmax function?\nAnswer: Softmax converts a vector of values into a probability distribution that sums to 1. It's used in multi-class classification output layers, exponentiating each element and normalizing by the sum."}
{"text": "What is semantic segmentation?\n\nSemantic segmentation assigns a class label to every pixel in an image, creating dense predictions. It's used for tasks like medical image analysis, autonomous driving, and scene understanding where precise boundaries matter."}
{"text": "Question: What is a recurrent neural network?\nAnswer: RNNs process sequential data by maintaining hidden states that carry information across time steps. Each step's output depends on current input and previous hidden state, enabling the network to capture temporal dependencies in sequences."}
{"text": "Question: What is the exploding gradient problem?\nAnswer: The exploding gradient problem occurs when gradients grow exponentially large during backpropagation, causing numerical instability. It's common in RNNs with long sequences and can be mitigated with gradient clipping, careful initialization, or architecture changes."}
{"text": "What are the disadvantages of LoRA?\n\nLoRA adds inference latency if not merged with base weights, requires careful hyperparameter tuning (rank, alpha), and may underperform full fine-tuning on tasks requiring substantial behavior changes. The optimal rank varies across tasks and models."}
{"text": "User: What is a test set?\nAssistant: A test set is data completely held out from training and validation, used only for final model evaluation. It provides an unbiased estimate of how the model will perform on new, unseen data in production."}
{"text": "Q: Explain LSTM networks.\nA: Long Short-Term Memory (LSTM) networks address RNN's vanishing gradient problem using gating mechanisms (input, forget, output gates) and memory cells. They can learn long-range dependencies by selectively remembering or forgetting information."}
{"text": "Question: What is perplexity?\nAnswer: Perplexity measures how well a probability model predicts samples, commonly used for language models. It's the exponential of cross-entropy loss. Lower perplexity indicates better predictions, with perplexity of 1 meaning perfect prediction."}
{"text": "Q: What is grid search?\nA: Grid search exhaustively evaluates model performance across all combinations of predefined hyperparameter values. While thorough, it's computationally expensive and suffers from the curse of dimensionality as the number of hyperparameters grows."}
{"text": "Q: What is the confusion matrix?\nA: A confusion matrix tabulates classification results showing true positives, false positives, true negatives, and false negatives. It provides detailed insight into model performance, revealing which classes are confused and error patterns."}
{"text": "Question: What modules should be targeted with LoRA?\nAnswer: LoRA typically targets attention weight matrices (Query, Key, Value, Output projections) as they contain most parameters and are most important for adaptation. Some implementations also target feed-forward network layers for additional capacity."}
{"text": "Q: How do you merge LoRA weights?\nA: LoRA weights can be merged into base model weights after training: W_merged = W_base + \u03b1 \u00d7 A \u00d7 B. This eliminates inference overhead from adapter modules while preserving task-specific adaptations in a single model that can be deployed normally."}
{"text": "Question: What is LoRA?\nAnswer: LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning technique that decomposes weight updates into low-rank matrices. Instead of updating weight W directly, it learns W + AB where A and B are much smaller, reducing trainable parameters by orders of magnitude."}
{"text": "User: What is unsupervised learning?\nAssistant: Unsupervised learning discovers hidden patterns in unlabeled data without predefined categories. The algorithm explores data structure through techniques like clustering, dimensionality reduction, and density estimation, revealing natural groupings or relationships."}
{"text": "Question: What is batch normalization?\nAnswer: Batch normalization normalizes layer inputs across mini-batches to have zero mean and unit variance. This reduces internal covariate shift, stabilizes training, allows higher learning rates, and acts as a regularizer, significantly accelerating deep network training."}
{"text": "Q: Why is LoRA effective?\nA: LoRA is effective because weight updates during fine-tuning often have low intrinsic dimensionality. By constraining updates to low-rank subspaces, it captures essential adaptations while dramatically reducing parameters. This also acts as regularization, preventing overfitting."}
{"text": "What is the sigmoid activation function?\n\nThe sigmoid function maps inputs to (0,1) using the formula 1/(1+e^-x). It's useful for binary classification output layers and was historically popular for hidden layers, though it suffers from vanishing gradients."}
{"text": "Q: What is prefix tuning?\nA: Prefix tuning prepends learnable continuous vectors (virtual tokens) to input sequences at each layer. These prefix parameters are optimized while the entire pre-trained model stays frozen, guiding model behavior for specific tasks with minimal parameters."}
{"text": "Question: What is k-fold cross-validation?\nAnswer: K-fold cross-validation divides data into k equal parts (folds). The model trains k times, each time using a different fold for validation and the remaining k-1 for training. Common choices are k=5 or k=10."}
{"text": "Explain stride in convolution.\n\nStride determines the step size when sliding filters across inputs. Stride 1 moves one pixel at a time; stride 2 skips every other position. Larger strides reduce output dimensions and computational cost but may miss fine details."}
{"text": "Q: What is instance segmentation?\nA: Instance segmentation distinguishes between different instances of the same class, combining object detection and semantic segmentation. It assigns unique labels to each object instance, useful for counting or tracking individual objects."}
{"text": "Q: What is a filter/kernel?\nA: A filter (or kernel) is a small matrix of learnable weights that slides across inputs in convolutional layers. It detects specific patterns through element-wise multiplication and summation, producing high responses where patterns match."}
{"text": "Q: What is learning rate warmup?\nA: Learning rate warmup gradually increases the learning rate from zero to the target value over initial training steps. This prevents destabilization from large gradients early in training when the model is far from optimal, especially important for large batch sizes."}
{"text": "What modules should be targeted with LoRA?\n\nLoRA typically targets attention weight matrices (Query, Key, Value, Output projections) as they contain most parameters and are most important for adaptation. Some implementations also target feed-forward network layers for additional capacity."}
{"text": "What is momentum in optimization?\n\nMomentum accelerates gradient descent by accumulating a velocity vector in directions of persistent gradient reduction. It helps overcome local minima, dampens oscillations, and speeds convergence, especially in ravines or plateaus."}
{"text": "User: What is backpropagation?\nAssistant: Backpropagation is the algorithm for training neural networks. It computes gradients of the loss function with respect to each weight by propagating errors backward through the network using the chain rule, enabling gradient descent optimization."}
{"text": "Q: What is sentiment analysis?\nA: Sentiment analysis determines the emotional tone or opinion expressed in text (positive, negative, neutral). It's widely used for analyzing customer feedback, social media monitoring, and market research."}
{"text": "Question: What is soft prompting?\nAnswer: Soft prompting learns continuous prompt embeddings in the model's embedding space rather than discrete text prompts. Unlike hard prompts (actual text), soft prompts can occupy regions of embedding space not reachable by vocabulary tokens, providing more flexibility."}
{"text": "User: Why is LoRA effective?\nAssistant: LoRA is effective because weight updates during fine-tuning often have low intrinsic dimensionality. By constraining updates to low-rank subspaces, it captures essential adaptations while dramatically reducing parameters. This also acts as regularization, preventing overfitting."}
{"text": "User: What is the transformer architecture?\nAssistant: Transformers process sequences using self-attention mechanisms instead of recurrence. They enable parallel computation, capture long-range dependencies efficiently, and form the foundation of modern NLP models like BERT, GPT, and T5."}
{"text": "What is soft prompting?\n\nSoft prompting learns continuous prompt embeddings in the model's embedding space rather than discrete text prompts. Unlike hard prompts (actual text), soft prompts can occupy regions of embedding space not reachable by vocabulary tokens, providing more flexibility."}
{"text": "User: What is prefix tuning?\nAssistant: Prefix tuning prepends learnable continuous vectors (virtual tokens) to input sequences at each layer. These prefix parameters are optimized while the entire pre-trained model stays frozen, guiding model behavior for specific tasks with minimal parameters."}
{"text": "Q: What is stacking?\nA: Stacking (Stacked Generalization) trains a meta-model to combine predictions from multiple base models. The meta-model learns optimal weights or combinations of base predictions, often outperforming simple averaging."}
{"text": "Question: What is mean average precision?\nAnswer: Mean Average Precision (mAP) evaluates object detection and information retrieval by averaging precision at different recall levels. It accounts for both detection accuracy and localization quality, providing a comprehensive performance metric."}
{"text": "What is data augmentation in images?\n\nData augmentation artificially increases training data diversity by applying transformations like rotations, flips, crops, color jitter, and noise. This improves model robustness, reduces overfitting, and teaches invariance to common variations."}
{"text": "Question: What is stratified cross-validation?\nAnswer: Stratified cross-validation ensures each fold has approximately the same class distribution as the original dataset. This is crucial for imbalanced datasets to ensure representative training and validation sets in each fold."}
{"text": "Q: What is stratified cross-validation?\nA: Stratified cross-validation ensures each fold has approximately the same class distribution as the original dataset. This is crucial for imbalanced datasets to ensure representative training and validation sets in each fold."}
{"text": "Q: What is tokenization?\nA: Tokenization breaks text into smaller units (tokens) such as words, subwords, or characters. It's the first step in NLP pipelines, converting raw text into structured units that models can process numerically."}
{"text": "User: What is learning rate warmup?\nAssistant: Learning rate warmup gradually increases the learning rate from zero to the target value over initial training steps. This prevents destabilization from large gradients early in training when the model is far from optimal, especially important for large batch sizes."}
{"text": "Q: What is ResNet?\nA: ResNet introduces skip connections (residual connections) that add layer inputs directly to outputs. This enables training very deep networks (50-152+ layers) by allowing gradients to flow directly through the network, solving degradation problems."}
{"text": "What is underfitting?\n\nUnderfitting happens when a model is too simple to capture underlying data patterns. It performs poorly on both training and test data because it lacks the capacity to learn the true relationship between inputs and outputs."}
{"text": "Q: What is cosine annealing?\nA: Cosine annealing decreases learning rate following a cosine curve from initial to minimum value over a period. It provides smooth decay and can be restarted periodically (warm restarts) to escape local minima and improve optimization."}
{"text": "Q: What are the advantages of LoRA over full fine-tuning?\nA: LoRA requires 10-100x fewer trainable parameters, drastically reducing memory usage and training time. It enables fine-tuning large models on consumer hardware, allows storing multiple task-specific adapters cheaply, and provides implicit regularization against overfitting."}
{"text": "What is the softmax function?\n\nSoftmax converts a vector of values into a probability distribution that sums to 1. It's used in multi-class classification output layers, exponentiating each element and normalizing by the sum."}
{"text": "Question: What is a validation set?\nAnswer: A validation set is data held out from training to tune hyperparameters and make model selection decisions. It provides unbiased evaluation during development without touching the test set, preventing overfitting to test data."}
{"text": "User: What is the softmax function?\nAssistant: Softmax converts a vector of values into a probability distribution that sums to 1. It's used in multi-class classification output layers, exponentiating each element and normalizing by the sum."}
{"text": "Question: What is momentum in optimization?\nAnswer: Momentum accelerates gradient descent by accumulating a velocity vector in directions of persistent gradient reduction. It helps overcome local minima, dampens oscillations, and speeds convergence, especially in ravines or plateaus."}
{"text": "User: What modules should be targeted with LoRA?\nAssistant: LoRA typically targets attention weight matrices (Query, Key, Value, Output projections) as they contain most parameters and are most important for adaptation. Some implementations also target feed-forward network layers for additional capacity."}
{"text": "Q: What is catastrophic forgetting?\nA: Catastrophic forgetting occurs when a neural network abruptly loses previously learned knowledge upon learning new information. It's a major challenge in continual learning, as updating weights for new tasks can overwrite weights important for old tasks."}
{"text": "Question: What is unsupervised learning?\nAnswer: Unsupervised learning discovers hidden patterns in unlabeled data without predefined categories. The algorithm explores data structure through techniques like clustering, dimensionality reduction, and density estimation, revealing natural groupings or relationships."}
{"text": "Q: What is mean average precision?\nA: Mean Average Precision (mAP) evaluates object detection and information retrieval by averaging precision at different recall levels. It accounts for both detection accuracy and localization quality, providing a comprehensive performance metric."}
{"text": "Question: What is the vanishing gradient problem?\nAnswer: The vanishing gradient problem occurs when gradients become exponentially small during backpropagation through many layers. This happens with saturating activations (sigmoid/tanh) and prevents deep network training as early layers receive tiny updates."}
{"text": "User: What is self-attention?\nAssistant: Self-attention computes attention within a single sequence, relating different positions to each other. Each element attends to all elements (including itself), learning which parts are relevant for representing each position in context."}
{"text": "Q: What is machine learning?\nA: Machine learning is a branch of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed. It uses algorithms to identify patterns in data and make predictions or decisions based on those patterns."}
{"text": "What is natural language processing?\n\nNatural language processing (NLP) enables computers to understand, interpret, and generate human language. It combines linguistics, computer science, and machine learning to process text and speech for tasks like translation, sentiment analysis, and question answering."}
{"text": "Q: What is word embedding?\nA: Word embeddings represent words as dense, low-dimensional vectors that capture semantic relationships. Words with similar meanings have similar vectors, enabling models to understand semantic similarity and perform arithmetic with meaning (king - man + woman \u2248 queen)."}
{"text": "User: What is boosting?\nAssistant: Boosting sequentially trains weak learners, each focusing on examples the previous models struggled with. It combines these weak learners into a strong ensemble that often achieves excellent performance. Examples include AdaBoost, Gradient Boosting, and XGBoost."}
{"text": "What is Xavier initialization?\n\nXavier (Glorot) initialization sets weights from a distribution with variance 1/n_in (or 2/(n_in+n_out)), where n_in and n_out are input and output dimensions. It maintains activation and gradient variance across layers for stable training with tanh/sigmoid."}
{"text": "User: What is the rank in LoRA?\nAssistant: The rank (r) in LoRA determines the dimensionality of the low-rank decomposition. Lower rank means fewer parameters and faster training but less expressiveness. Higher rank provides more capacity but increases cost. Typical values range from 4 to 64."}
{"text": "Question: What is backpropagation?\nAnswer: Backpropagation is the algorithm for training neural networks. It computes gradients of the loss function with respect to each weight by propagating errors backward through the network using the chain rule, enabling gradient descent optimization."}
{"text": "What is hyperparameter tuning?\n\nHyperparameter tuning systematically searches for optimal hyperparameter values. Methods include grid search (exhaustive), random search (efficient), and Bayesian optimization (smart). Good tuning can dramatically improve model performance."}
{"text": "What is question answering?\n\nQuestion answering systems take a question and context (or knowledge base) and generate or extract an answer. It combines reading comprehension, information retrieval, and natural language understanding."}
{"text": "Question: What is the intrinsic dimensionality hypothesis?\nAnswer: The intrinsic dimensionality hypothesis posits that successful model updates lie in a much lower-dimensional subspace than the full parameter space. This explains why low-rank methods like LoRA work well: the essential adaptations fit in small subspaces."}
{"text": "Q: What is the learning rate?\nA: The learning rate controls the step size in gradient descent optimization. Too high causes divergence or oscillation around minima; too low results in slow convergence or getting stuck. It's one of the most critical hyperparameters requiring careful tuning."}
{"text": "Q: What is GPT?\nA: GPT (Generative Pre-trained Transformer) is an autoregressive language model that predicts next tokens given previous context. It's pre-trained on large text corpora and can be fine-tuned for various tasks or used for few-shot learning."}
{"text": "User: What is feature selection?\nAssistant: Feature selection identifies and retains only the most relevant features for a task, removing irrelevant or redundant ones. This reduces dimensionality, improves model interpretability, decreases training time, and can prevent overfitting."}
{"text": "What is object detection?\n\nObject detection identifies and localizes multiple objects in images, predicting both class labels and bounding boxes. It's more challenging than classification as it requires determining what objects exist and where they are located."}
{"text": "What is a neural network?\n\nA neural network is a computational model inspired by biological neurons, consisting of interconnected nodes (artificial neurons) organized in layers. It learns to map inputs to outputs through adjustable weights that are optimized during training."}
{"text": "Q: What is object detection?\nA: Object detection identifies and localizes multiple objects in images, predicting both class labels and bounding boxes. It's more challenging than classification as it requires determining what objects exist and where they are located."}
{"text": "Q: Explain adapter layers.\nA: Adapter layers are small bottleneck modules inserted into pre-trained models. They consist of down-projection, non-linearity, and up-projection layers. Only adapter parameters are trained while original weights stay frozen, enabling efficient multi-task learning."}
{"text": "Question: What is machine learning?\nAnswer: Machine learning is a branch of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed. It uses algorithms to identify patterns in data and make predictions or decisions based on those patterns."}
{"text": "Question: What is early stopping?\nAnswer: Early stopping halts training when validation performance stops improving for a specified number of epochs (patience). It prevents overfitting by finding the point where the model generalizes best before it starts memorizing training data."}
{"text": "Q: What is boosting?\nA: Boosting sequentially trains weak learners, each focusing on examples the previous models struggled with. It combines these weak learners into a strong ensemble that often achieves excellent performance. Examples include AdaBoost, Gradient Boosting, and XGBoost."}
{"text": "User: What are feature maps?\nAssistant: Feature maps are the outputs of convolutional or pooling layers, representing the presence and strength of learned features at different spatial locations. Early layers detect simple features (edges), while deeper layers detect complex patterns (objects)."}
{"text": "Q: What is weight decay?\nA: Weight decay adds a penalty proportional to weight magnitudes to the loss function, encouraging smaller weights. It's equivalent to L2 regularization and prevents overfitting by discouraging complex models. Typical values are 0.0001 to 0.01."}
{"text": "Question: Explain cross-validation.\nAnswer: Cross-validation assesses model performance by partitioning data into k folds, iteratively training on k-1 folds and validating on the remaining fold. Results are averaged across all folds, providing robust performance estimates and reducing variance."}
{"text": "User: What is recall?\nAssistant: Recall (sensitivity) is the fraction of true positives among all actual positives: TP / (TP + FN). It answers 'Of all actual positive instances, how many did we find?' High recall means few false negatives."}
{"text": "Q: What is next sentence prediction?\nA: Next sentence prediction is a BERT pre-training task that determines if two sentences are consecutive in the original text. It helps the model learn sentence-level relationships useful for tasks like question answering and natural language inference."}
{"text": "Q: What is random search?\nA: Random search samples hyperparameter combinations randomly from specified distributions. It's more efficient than grid search, especially when some hyperparameters don't significantly affect performance, and can discover better configurations with fewer evaluations."}
{"text": "User: What is weight decay?\nAssistant: Weight decay adds a penalty proportional to weight magnitudes to the loss function, encouraging smaller weights. It's equivalent to L2 regularization and prevents overfitting by discouraging complex models. Typical values are 0.0001 to 0.01."}
{"text": "User: What is grid search?\nAssistant: Grid search exhaustively evaluates model performance across all combinations of predefined hyperparameter values. While thorough, it's computationally expensive and suffers from the curse of dimensionality as the number of hyperparameters grows."}
{"text": "User: Explain cross-validation.\nAssistant: Cross-validation assesses model performance by partitioning data into k folds, iteratively training on k-1 folds and validating on the remaining fold. Results are averaged across all folds, providing robust performance estimates and reducing variance."}
{"text": "Why is LoRA effective?\n\nLoRA is effective because weight updates during fine-tuning often have low intrinsic dimensionality. By constraining updates to low-rank subspaces, it captures essential adaptations while dramatically reducing parameters. This also acts as regularization, preventing overfitting."}
{"text": "Question: Explain supervised learning.\nAnswer: Supervised learning is a machine learning paradigm where models learn from labeled training data. The algorithm learns a mapping from inputs to outputs using examples, then applies this learned mapping to make predictions on new, unseen data. Common tasks include classification and regression."}
{"text": "Explain Word2Vec.\n\nWord2Vec learns word embeddings by predicting words from context (CBOW) or context from words (Skip-gram). It trains a shallow neural network on large corpora, producing embeddings where semantically similar words cluster together in vector space."}
{"text": "Q: Explain cross-entropy loss.\nA: Cross-entropy loss measures the difference between predicted probability distributions and true labels. For classification, it's the negative log probability of the correct class. It's the standard loss for classification tasks."}
{"text": "User: What is natural language processing?\nAssistant: Natural language processing (NLP) enables computers to understand, interpret, and generate human language. It combines linguistics, computer science, and machine learning to process text and speech for tasks like translation, sentiment analysis, and question answering."}
{"text": "What is transfer learning in computer vision?\n\nTransfer learning uses models pre-trained on large datasets (like ImageNet) as starting points for new tasks. The network has already learned useful features, requiring less data and training time for downstream tasks through fine-tuning."}
{"text": "What is a hyperparameter?\n\nHyperparameters are configuration settings that control the learning process but aren't learned from data. Examples include learning rate, batch size, number of layers, and regularization strength. They must be set before training and tuned for optimal performance."}
{"text": "What is He initialization?\n\nHe initialization uses variance 2/n_in, doubling Xavier's variance. It's designed for ReLU activations which kill half the gradients. This compensation maintains stable gradient flow through deep ReLU networks."}
{"text": "Question: What is a convolutional neural network?\nAnswer: A CNN is a neural network specialized for processing grid-structured data like images. It uses convolutional layers to detect local patterns through learnable filters, pooling layers to reduce spatial dimensions, and fully connected layers for classification."}
{"text": "User: What is perplexity?\nAssistant: Perplexity measures how well a probability model predicts samples, commonly used for language models. It's the exponential of cross-entropy loss. Lower perplexity indicates better predictions, with perplexity of 1 meaning perfect prediction."}
{"text": "Question: What is self-attention?\nAnswer: Self-attention computes attention within a single sequence, relating different positions to each other. Each element attends to all elements (including itself), learning which parts are relevant for representing each position in context."}
{"text": "What is recall?\n\nRecall (sensitivity) is the fraction of true positives among all actual positives: TP / (TP + FN). It answers 'Of all actual positive instances, how many did we find?' High recall means few false negatives."}
{"text": "Q: What is fine-tuning in machine learning?\nA: Fine-tuning adapts a pre-trained model to a specific task by continuing training on task-specific data. It leverages knowledge learned from large datasets, requiring less data and computation than training from scratch while achieving better performance."}
{"text": "Question: What is the F1 score?\nAnswer: The F1 score is the harmonic mean of precision and recall: 2 \u00d7 (precision \u00d7 recall) / (precision + recall). It balances both metrics, useful when you need a single score accounting for both false positives and false negatives."}
{"text": "User: What is gradient accumulation?\nAssistant: Gradient accumulation computes gradients over multiple mini-batches before updating weights. This simulates larger batch sizes without increased memory, as gradients are summed across batches then averaged for the update. Useful for memory-constrained training."}
{"text": "User: What is bagging?\nAssistant: Bagging (Bootstrap Aggregating) trains multiple models on different random samples of training data (with replacement) and averages their predictions. It reduces variance and prevents overfitting, with Random Forests being a popular example."}
{"text": "Q: Explain cross-validation.\nA: Cross-validation assesses model performance by partitioning data into k folds, iteratively training on k-1 folds and validating on the remaining fold. Results are averaged across all folds, providing robust performance estimates and reducing variance."}
{"text": "Q: What is a training set?\nA: A training set is the portion of data used to train a machine learning model. The model learns patterns, relationships, and parameters from this data. It's typically the largest split, often 60-80% of total data."}
{"text": "User: Explain the curse of dimensionality.\nAssistant: The curse of dimensionality refers to phenomena that arise when working with high-dimensional data. As dimensions increase, data becomes sparse, distances lose meaning, and exponentially more samples are needed to maintain density and statistical significance."}
{"text": "Q: What are residual connections?\nA: Residual (skip) connections add layer inputs to outputs, learning residual mappings F(x) instead of desired mappings H(x) = F(x) + x. This enables training very deep networks by providing gradient highways and making learning easier."}
{"text": "Q: What is semantic segmentation?\nA: Semantic segmentation assigns a class label to every pixel in an image, creating dense predictions. It's used for tasks like medical image analysis, autonomous driving, and scene understanding where precise boundaries matter."}
{"text": "User: What is leave-one-out cross-validation?\nAssistant: Leave-one-out cross-validation (LOOCV) is an extreme case of k-fold CV where k equals the dataset size. Each iteration uses one sample for validation and all others for training. It's computationally expensive but provides maximum training data per iteration."}
{"text": "User: Can multiple LoRA adapters be used together?\nAssistant: Yes, multiple LoRA adapters can be combined for different tasks or aspects. They can be weighted and summed: W = W_base + \u03b1\u2081A\u2081B\u2081 + \u03b1\u2082A\u2082B\u2082. This enables multi-task models, mixing capabilities, or compositional task solving."}
{"text": "User: What is gradient clipping?\nAssistant: Gradient clipping caps gradient magnitudes to a maximum value, preventing exploding gradients. It's essential for RNN training and stabilizes optimization when gradients occasionally spike. Common thresholds are clip by norm or clip by value."}
{"text": "Q: What is a convolution operation?\nA: Convolution slides a small filter (kernel) across an input, computing element-wise multiplication and summing results at each position. This creates a feature map indicating filter response strength across spatial locations, detecting local patterns."}
{"text": "User: What is GRU?\nAssistant: Gated Recurrent Units (GRU) simplify LSTMs by combining forget and input gates into an update gate and merging cell and hidden states. They achieve similar performance to LSTMs with fewer parameters and faster training."}
{"text": "What is average pooling?\n\nAverage pooling computes the mean of values within each pooling window. It provides smoother downsampling than max pooling and is sometimes used in network final layers to aggregate spatial information."}
{"text": "What is GRU?\n\nGated Recurrent Units (GRU) simplify LSTMs by combining forget and input gates into an update gate and merging cell and hidden states. They achieve similar performance to LSTMs with fewer parameters and faster training."}
{"text": "User: What is LoRA alpha?\nAssistant: LoRA alpha (\u03b1) is a scaling hyperparameter that controls the magnitude of LoRA updates relative to original weights. The effective learning rate for LoRA parameters is scaled by \u03b1/r, where r is rank. Common practice sets \u03b1 = 2r."}
{"text": "User: What is mean squared error?\nAssistant: Mean squared error (MSE) averages the squared differences between predictions and targets. It's the standard loss function for regression, penalizing large errors more heavily than small ones due to squaring."}
{"text": "What is tokenization?\n\nTokenization breaks text into smaller units (tokens) such as words, subwords, or characters. It's the first step in NLP pipelines, converting raw text into structured units that models can process numerically."}
{"text": "What is GloVe?\n\nGloVe (Global Vectors) learns embeddings by factorizing word co-occurrence matrices. It captures both local context and global corpus statistics, combining benefits of matrix factorization and local context window methods like Word2Vec."}
{"text": "Question: What is LoRA alpha?\nAnswer: LoRA alpha (\u03b1) is a scaling hyperparameter that controls the magnitude of LoRA updates relative to original weights. The effective learning rate for LoRA parameters is scaled by \u03b1/r, where r is rank. Common practice sets \u03b1 = 2r."}
{"text": "User: What is a loss function?\nAssistant: A loss function (or cost function) quantifies the difference between model predictions and true labels. It provides a differentiable objective that optimization algorithms minimize during training to improve model performance."}
{"text": "What is leave-one-out cross-validation?\n\nLeave-one-out cross-validation (LOOCV) is an extreme case of k-fold CV where k equals the dataset size. Each iteration uses one sample for validation and all others for training. It's computationally expensive but provides maximum training data per iteration."}
{"text": "What is label smoothing?\n\nLabel smoothing replaces hard 0/1 targets with soft targets (e.g., 0.9/0.1), reducing model overconfidence. It acts as regularization, preventing overfitting and improving model calibration, especially important for classification with many classes."}
{"text": "Question: Explain ensemble learning.\nAnswer: Ensemble learning combines multiple models to produce better predictions than any individual model. By aggregating diverse models' outputs, ensembles reduce variance, increase robustness, and often achieve superior performance."}
{"text": "User: What is padding in CNNs?\nAssistant: Padding adds border pixels (usually zeros) around inputs before convolution. This preserves spatial dimensions, allows filters to process edge pixels effectively, and prevents rapid shrinking of feature maps through network depth."}
{"text": "Question: What is next sentence prediction?\nAnswer: Next sentence prediction is a BERT pre-training task that determines if two sentences are consecutive in the original text. It helps the model learn sentence-level relationships useful for tasks like question answering and natural language inference."}
{"text": "User: Explain convolutional layers.\nAssistant: Convolutional layers apply learnable filters (kernels) to input feature maps through convolution operations. Each filter detects specific patterns (edges, textures, objects) and produces feature maps showing where those patterns occur spatially."}
{"text": "What are residual connections?\n\nResidual (skip) connections add layer inputs to outputs, learning residual mappings F(x) instead of desired mappings H(x) = F(x) + x. This enables training very deep networks by providing gradient highways and making learning easier."}
{"text": "What is named entity recognition?\n\nNamed Entity Recognition (NER) identifies and classifies named entities (people, organizations, locations, dates) in text. It's a fundamental NLP task used in information extraction, question answering, and knowledge graph construction."}
{"text": "Question: What are the advantages of LoRA over full fine-tuning?\nAnswer: LoRA requires 10-100x fewer trainable parameters, drastically reducing memory usage and training time. It enables fine-tuning large models on consumer hardware, allows storing multiple task-specific adapters cheaply, and provides implicit regularization against overfitting."}
{"text": "User: What is label smoothing?\nAssistant: Label smoothing replaces hard 0/1 targets with soft targets (e.g., 0.9/0.1), reducing model overconfidence. It acts as regularization, preventing overfitting and improving model calibration, especially important for classification with many classes."}
{"text": "What is LoRA?\n\nLoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning technique that decomposes weight updates into low-rank matrices. Instead of updating weight W directly, it learns W + AB where A and B are much smaller, reducing trainable parameters by orders of magnitude."}
{"text": "User: What is a recurrent neural network?\nAssistant: RNNs process sequential data by maintaining hidden states that carry information across time steps. Each step's output depends on current input and previous hidden state, enabling the network to capture temporal dependencies in sequences."}
{"text": "Q: Explain the ReLU activation function.\nA: ReLU (Rectified Linear Unit) outputs max(0, x), zero for negative inputs and identity for positive. It's computationally efficient, mitigates vanishing gradients, and induces sparsity. It's the most popular activation for hidden layers in deep networks."}
{"text": "Question: What is a test set?\nAnswer: A test set is data completely held out from training and validation, used only for final model evaluation. It provides an unbiased estimate of how the model will perform on new, unseen data in production."}
{"text": "Question: What is mean absolute error?\nAnswer: Mean absolute error (MAE) averages the absolute differences between predictions and targets. Unlike MSE, it treats all errors equally and is more robust to outliers, making it useful when large errors should not be overly penalized."}
{"text": "Question: What is a convolution operation?\nAnswer: Convolution slides a small filter (kernel) across an input, computing element-wise multiplication and summing results at each position. This creates a feature map indicating filter response strength across spatial locations, detecting local patterns."}
{"text": "Question: What is the Adam optimizer?\nAnswer: Adam (Adaptive Moment Estimation) combines momentum and adaptive learning rates per parameter. It maintains exponential moving averages of gradients and squared gradients, adapting learning rates based on first and second moment estimates for efficient optimization."}
{"text": "User: What is the F1 score?\nAssistant: The F1 score is the harmonic mean of precision and recall: 2 \u00d7 (precision \u00d7 recall) / (precision + recall). It balances both metrics, useful when you need a single score accounting for both false positives and false negatives."}
{"text": "Q: What is the bias-variance tradeoff?\nA: The bias-variance tradeoff describes the balance between a model's ability to fit training data (low bias) and generalize to new data (low variance). High bias leads to underfitting, high variance to overfitting. Optimal models balance both."}
{"text": "Question: What is the tanh activation function?\nAnswer: The tanh (hyperbolic tangent) function maps inputs to (-1,1). It's zero-centered, making optimization easier than sigmoid, but also suffers from vanishing gradients. It's sometimes used in RNNs and specific architectures."}
{"text": "Q: What is a validation set?\nA: A validation set is data held out from training to tune hyperparameters and make model selection decisions. It provides unbiased evaluation during development without touching the test set, preventing overfitting to test data."}
{"text": "Question: What is prefix tuning?\nAnswer: Prefix tuning prepends learnable continuous vectors (virtual tokens) to input sequences at each layer. These prefix parameters are optimized while the entire pre-trained model stays frozen, guiding model behavior for specific tasks with minimal parameters."}
{"text": "Question: Explain adapter layers.\nAnswer: Adapter layers are small bottleneck modules inserted into pre-trained models. They consist of down-projection, non-linearity, and up-projection layers. Only adapter parameters are trained while original weights stay frozen, enabling efficient multi-task learning."}
{"text": "User: What is the vanishing gradient problem?\nAssistant: The vanishing gradient problem occurs when gradients become exponentially small during backpropagation through many layers. This happens with saturating activations (sigmoid/tanh) and prevents deep network training as early layers receive tiny updates."}
{"text": "User: What is fine-tuning in machine learning?\nAssistant: Fine-tuning adapts a pre-trained model to a specific task by continuing training on task-specific data. It leverages knowledge learned from large datasets, requiring less data and computation than training from scratch while achieving better performance."}
