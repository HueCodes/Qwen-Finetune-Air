{"text": "User: What is ROC curve?\nAssistant: The ROC (Receiver Operating Characteristic) curve plots true positive rate (recall) against false positive rate across all classification thresholds. It visualizes the tradeoff between sensitivity and specificity independent of class distribution."}
{"text": "Question: What is GloVe?\nAnswer: GloVe (Global Vectors) learns embeddings by factorizing word co-occurrence matrices. It captures both local context and global corpus statistics, combining benefits of matrix factorization and local context window methods like Word2Vec."}
{"text": "Q: Explain transfer learning.\nA: Transfer learning applies knowledge from models trained on one task to improve performance on different but related tasks. Pre-trained models capture useful features and representations that transfer across domains, enabling efficient learning with limited data."}
{"text": "What is masked language modeling?\n\nMasked language modeling randomly masks tokens in text and trains models to predict masked tokens from context. BERT uses this self-supervised objective to learn bidirectional representations from unlabeled text at scale."}
{"text": "User: What is mean absolute error?\nAssistant: Mean absolute error (MAE) averages the absolute differences between predictions and targets. Unlike MSE, it treats all errors equally and is more robust to outliers, making it useful when large errors should not be overly penalized."}
{"text": "What is early stopping?\n\nEarly stopping halts training when validation performance stops improving for a specified number of epochs (patience). It prevents overfitting by finding the point where the model generalizes best before it starts memorizing training data."}
{"text": "Q: What is transfer learning in computer vision?\nA: Transfer learning uses models pre-trained on large datasets (like ImageNet) as starting points for new tasks. The network has already learned useful features, requiring less data and training time for downstream tasks through fine-tuning."}
{"text": "Question: What is the confusion matrix?\nAnswer: A confusion matrix tabulates classification results showing true positives, false positives, true negatives, and false negatives. It provides detailed insight into model performance, revealing which classes are confused and error patterns."}
{"text": "Question: What is a loss function?\nAnswer: A loss function (or cost function) quantifies the difference between model predictions and true labels. It provides a differentiable objective that optimization algorithms minimize during training to improve model performance."}
{"text": "Q: What is label smoothing?\nA: Label smoothing replaces hard 0/1 targets with soft targets (e.g., 0.9/0.1), reducing model overconfidence. It acts as regularization, preventing overfitting and improving model calibration, especially important for classification with many classes."}
{"text": "Question: What is tokenization?\nAnswer: Tokenization breaks text into smaller units (tokens) such as words, subwords, or characters. It's the first step in NLP pipelines, converting raw text into structured units that models can process numerically."}
{"text": "What is stacking?\n\nStacking (Stacked Generalization) trains a meta-model to combine predictions from multiple base models. The meta-model learns optimal weights or combinations of base predictions, often outperforming simple averaging."}
{"text": "Q: What is early stopping?\nA: Early stopping halts training when validation performance stops improving for a specified number of epochs (patience). It prevents overfitting by finding the point where the model generalizes best before it starts memorizing training data."}
{"text": "What is the transformer architecture?\n\nTransformers process sequences using self-attention mechanisms instead of recurrence. They enable parallel computation, capture long-range dependencies efficiently, and form the foundation of modern NLP models like BERT, GPT, and T5."}
{"text": "Q: What is overfitting?\nA: Overfitting occurs when a model learns training data too well, including noise and outliers. The model memorizes rather than generalizes, achieving high training accuracy but poor performance on new data. Signs include large gap between train and validation performance."}
{"text": "What is next sentence prediction?\n\nNext sentence prediction is a BERT pre-training task that determines if two sentences are consecutive in the original text. It helps the model learn sentence-level relationships useful for tasks like question answering and natural language inference."}
{"text": "Question: What is fine-tuning in machine learning?\nAnswer: Fine-tuning adapts a pre-trained model to a specific task by continuing training on task-specific data. It leverages knowledge learned from large datasets, requiring less data and computation than training from scratch while achieving better performance."}
{"text": "What is catastrophic forgetting?\n\nCatastrophic forgetting occurs when a neural network abruptly loses previously learned knowledge upon learning new information. It's a major challenge in continual learning, as updating weights for new tasks can overwrite weights important for old tasks."}
{"text": "Q: What is mean absolute error?\nA: Mean absolute error (MAE) averages the absolute differences between predictions and targets. Unlike MSE, it treats all errors equally and is more robust to outliers, making it useful when large errors should not be overly penalized."}
{"text": "What is machine translation?\n\nMachine translation automatically translates text from one language to another. Modern neural machine translation uses encoder-decoder architectures with attention, achieving near-human performance for some language pairs."}
{"text": "What is an activation function?\n\nAn activation function introduces non-linearity into neural networks, enabling them to learn complex patterns. It determines whether and how strongly a neuron fires based on its input. Examples include ReLU, sigmoid, and tanh."}
{"text": "Question: What is dropout?\nAnswer: Dropout randomly deactivates neurons during training with probability p (typically 0.5). This prevents co-adaptation of features, creates an ensemble effect by training different sub-networks, and serves as powerful regularization for deep networks."}
{"text": "Question: What is Xavier initialization?\nAnswer: Xavier (Glorot) initialization sets weights from a distribution with variance 1/n_in (or 2/(n_in+n_out)), where n_in and n_out are input and output dimensions. It maintains activation and gradient variance across layers for stable training with tanh/sigmoid."}
{"text": "What is boosting?\n\nBoosting sequentially trains weak learners, each focusing on examples the previous models struggled with. It combines these weak learners into a strong ensemble that often achieves excellent performance. Examples include AdaBoost, Gradient Boosting, and XGBoost."}
{"text": "User: What is weight initialization?\nAssistant: Weight initialization sets initial parameter values before training. Poor initialization can cause vanishing/exploding gradients or slow convergence. Modern methods like Xavier and He initialization account for layer sizes and activations for stable training."}
{"text": "What is the intrinsic dimensionality hypothesis?\n\nThe intrinsic dimensionality hypothesis posits that successful model updates lie in a much lower-dimensional subspace than the full parameter space. This explains why low-rank methods like LoRA work well: the essential adaptations fit in small subspaces."}
{"text": "Question: What is masked language modeling?\nAnswer: Masked language modeling randomly masks tokens in text and trains models to predict masked tokens from context. BERT uses this self-supervised objective to learn bidirectional representations from unlabeled text at scale."}
{"text": "Q: What is padding in CNNs?\nA: Padding adds border pixels (usually zeros) around inputs before convolution. This preserves spatial dimensions, allows filters to process edge pixels effectively, and prevents rapid shrinking of feature maps through network depth."}
{"text": "Question: What is recall?\nAnswer: Recall (sensitivity) is the fraction of true positives among all actual positives: TP / (TP + FN). It answers 'Of all actual positive instances, how many did we find?' High recall means few false negatives."}
{"text": "Question: What is accuracy?\nAnswer: Accuracy is the fraction of correct predictions: (TP + TN) / Total. While intuitive, it's misleading for imbalanced datasets where a naive baseline achieves high accuracy by always predicting the majority class."}
{"text": "What is feature engineering?\n\nFeature engineering is the process of creating new features or transforming existing ones to improve model performance. It involves domain knowledge to extract meaningful representations that make patterns more apparent to learning algorithms."}
{"text": "What is random search?\n\nRandom search samples hyperparameter combinations randomly from specified distributions. It's more efficient than grid search, especially when some hyperparameters don't significantly affect performance, and can discover better configurations with fewer evaluations."}
{"text": "What is the tanh activation function?\n\nThe tanh (hyperbolic tangent) function maps inputs to (-1,1). It's zero-centered, making optimization easier than sigmoid, but also suffers from vanishing gradients. It's sometimes used in RNNs and specific architectures."}
{"text": "Question: What is grid search?\nAnswer: Grid search exhaustively evaluates model performance across all combinations of predefined hyperparameter values. While thorough, it's computationally expensive and suffers from the curse of dimensionality as the number of hyperparameters grows."}
{"text": "User: What is mixed precision training?\nAssistant: Mixed precision training uses both FP16 (fast, memory-efficient) and FP32 (stable, precise) arithmetic. Activations and gradients use FP16 while master weights stay in FP32. This accelerates training and reduces memory with minimal quality loss."}
{"text": "Q: What is mini-batch gradient descent?\nA: Mini-batch gradient descent computes gradients on small batches of samples (e.g., 32-512), balancing between full-batch stability and stochastic speed. It's the most common variant used in practice, offering efficient parallelization and stable convergence."}
{"text": "Explain adapter layers.\n\nAdapter layers are small bottleneck modules inserted into pre-trained models. They consist of down-projection, non-linearity, and up-projection layers. Only adapter parameters are trained while original weights stay frozen, enabling efficient multi-task learning."}
{"text": "Q: What is the tanh activation function?\nA: The tanh (hyperbolic tangent) function maps inputs to (-1,1). It's zero-centered, making optimization easier than sigmoid, but also suffers from vanishing gradients. It's sometimes used in RNNs and specific architectures."}
{"text": "User: What is sentiment analysis?\nAssistant: Sentiment analysis determines the emotional tone or opinion expressed in text (positive, negative, neutral). It's widely used for analyzing customer feedback, social media monitoring, and market research."}
{"text": "Explain the difference between classification and regression.\n\nClassification predicts discrete categories or labels (e.g., cat vs dog), while regression predicts continuous numerical values (e.g., temperature). Both are supervised learning tasks but differ in output type and evaluation metrics."}
{"text": "What is a test set?\n\nA test set is data completely held out from training and validation, used only for final model evaluation. It provides an unbiased estimate of how the model will perform on new, unseen data in production."}
{"text": "Q: What is parameter-efficient fine-tuning?\nA: Parameter-efficient fine-tuning adapts large pre-trained models while updating only a small fraction of parameters. Methods like LoRA, adapters, and prefix tuning achieve comparable performance to full fine-tuning with drastically reduced memory and computational requirements."}
{"text": "User: What is a multilayer perceptron?\nAssistant: A multilayer perceptron (MLP) is a feedforward neural network with one or more hidden layers between input and output. Hidden layers with non-linear activations enable learning complex, non-linear relationships between inputs and outputs."}
{"text": "Explain LSTM networks.\n\nLong Short-Term Memory (LSTM) networks address RNN's vanishing gradient problem using gating mechanisms (input, forget, output gates) and memory cells. They can learn long-range dependencies by selectively remembering or forgetting information."}
{"text": "Question: What is weight initialization?\nAnswer: Weight initialization sets initial parameter values before training. Poor initialization can cause vanishing/exploding gradients or slow convergence. Modern methods like Xavier and He initialization account for layer sizes and activations for stable training."}
{"text": "Question: What is random search?\nAnswer: Random search samples hyperparameter combinations randomly from specified distributions. It's more efficient than grid search, especially when some hyperparameters don't significantly affect performance, and can discover better configurations with fewer evaluations."}
{"text": "Question: What is a learning rate schedule?\nAnswer: Learning rate schedules adjust the learning rate during training, typically decreasing it over time. Common strategies include step decay, exponential decay, and cosine annealing. Proper scheduling improves convergence and final performance."}
{"text": "Q: What is BERT?\nA: BERT (Bidirectional Encoder Representations from Transformers) pre-trains bidirectional transformers on large corpora using masked language modeling and next sentence prediction. It captures deep bidirectional context and achieves state-of-the-art results on many NLP tasks."}
{"text": "Question: What is GPT?\nAnswer: GPT (Generative Pre-trained Transformer) is an autoregressive language model that predicts next tokens given previous context. It's pre-trained on large text corpora and can be fine-tuned for various tasks or used for few-shot learning."}
{"text": "Q: What is the intrinsic dimensionality hypothesis?\nA: The intrinsic dimensionality hypothesis posits that successful model updates lie in a much lower-dimensional subspace than the full parameter space. This explains why low-rank methods like LoRA work well: the essential adaptations fit in small subspaces."}
{"text": "User: What is mini-batch gradient descent?\nAssistant: Mini-batch gradient descent computes gradients on small batches of samples (e.g., 32-512), balancing between full-batch stability and stochastic speed. It's the most common variant used in practice, offering efficient parallelization and stable convergence."}
{"text": "User: Explain Word2Vec.\nAssistant: Word2Vec learns word embeddings by predicting words from context (CBOW) or context from words (Skip-gram). It trains a shallow neural network on large corpora, producing embeddings where semantically similar words cluster together in vector space."}
{"text": "Q: What is underfitting?\nA: Underfitting happens when a model is too simple to capture underlying data patterns. It performs poorly on both training and test data because it lacks the capacity to learn the true relationship between inputs and outputs."}
