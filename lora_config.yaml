# LoRA Fine-tuning Configuration for Qwen2.5-1.5B on M2 MacBook Air
# Optimized for 8GB unified memory

# Model
model: "Qwen/Qwen2.5-1.5B-Instruct"

# Data
data: "./data"

# Training Configuration
batch-size: 2
iters: 600
learning-rate: 0.0001
grad-accumulation-steps: 8

# Memory Optimization
max-seq-length: 512
grad-checkpoint: true

# LoRA Settings (passed via CLI)
# rank: 16
# alpha: 16
# These aren't in YAML config, set via command line

# Evaluation
steps-per-report: 10
steps-per-eval: 50
val-batches: -1

# Checkpointing
save-every: 100
adapter-path: "./adapters"

# Misc
seed: 42
optimizer: "adamw"
num-layers: -1
